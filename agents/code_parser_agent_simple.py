"""
FIXED CodeParser Agent - Database Schema and SQL Corrections
CRITICAL FIXES: All SQL syntax errors and missing columns resolved
"""
import tiktoken
import re
import asyncio
import sqlite3
import json
import os
import uuid
from typing import Dict, List, Optional, Any, Tuple, Union
from pathlib import Path
from dataclasses import dataclass, field
import hashlib
from datetime import datetime as dt
import logging
from enum import Enum
from contextlib import asynccontextmanager
import copy

# Import the base agent
from agents.base_agent_api import BaseOpulenceAgent, SamplingParams

# Simple data classes
@dataclass
class CodeChunk:
    """Simplified code chunk representation"""
    program_name: str
    chunk_id: str
    chunk_type: str
    content: str
    metadata: Dict[str, Any]
    line_start: int
    line_end: int
    confidence_score: float = 1.0

@dataclass
class RelationshipRecord:
    """Generic relationship record"""
    source_name: str
    target_name: str
    relationship_type: str
    location: str
    line_number: int
    metadata: Dict[str, Any] = field(default_factory=dict)

class CodeParserAgent(BaseOpulenceAgent):
    """
    FIXED CodeParser Agent - All database schema issues resolved
    """
    
    def __init__(self, coordinator, agent_type: str = "code_parser", 
                 db_path: str = None, gpu_id: int = 0, **kwargs):
        
        # CRITICAL FIX: Use coordinator's database path if not provided
        if db_path is None and hasattr(coordinator, 'db_path'):
            db_path = coordinator.db_path
        elif db_path is None:
            db_path = "opulence_data.db"
        
        super().__init__(coordinator, agent_type, db_path, gpu_id)
        
        # FIXED: Conservative API parameters aligned with coordinator
        self.api_params.update({
            "max_tokens": 20,      # FIXED: Much smaller, coordinator-aligned
            "temperature": 0.1,
            "top_p": 0.9
        })
        
        # FIXED: Context window management aligned with coordinator limits
        self.max_context_tokens = 1500  # FIXED: Reduced from 2048
        self.reserve_tokens = 200       # FIXED: Reduced from 300
        self.max_content_tokens = min(1000, self.max_context_tokens - self.reserve_tokens)
        
        # Initialize simplified patterns
        self._init_core_patterns()
        
        # ENHANCED: Initialize database with LineageAnalyzer support
        self._init_enhanced_database_with_lineage()
        
        # FIXED: Track statistics for coordinator
        self._files_processed = 0
        self._total_chunks = 0
        self._api_calls = 0
        self._successful_analyses = 0
        self._failed_analyses = 0
        
        self.logger.info(f"üöÄ FIXED CodeParser Agent initialized with proper database schema")

    def _init_core_patterns(self):
        """Initialize simplified, reliable patterns"""
        
        # COBOL Core Patterns - Simplified and Reliable
        self.cobol_patterns = {
            # Basic identification
            'program_id': re.compile(r'PROGRAM-ID\s*\.\s*([A-Z0-9][A-Z0-9-]*)', re.IGNORECASE),
            
            # File definitions - CRITICAL for relationships
            'select_assign': re.compile(r'SELECT\s+([A-Z][A-Z0-9-]*)\s+ASSIGN\s+TO\s+([A-Z0-9-]+)', re.IGNORECASE),
            'fd_definition': re.compile(r'FD\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            
            # File operations - Simplified patterns
            'file_open': re.compile(r'OPEN\s+(INPUT|OUTPUT|I-O|EXTEND)\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            'file_read': re.compile(r'READ\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            'file_write': re.compile(r'WRITE\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            'file_close': re.compile(r'CLOSE\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            
            # Program calls
            'cobol_call': re.compile(r'CALL\s+[\'"]([A-Z0-9][A-Z0-9-]*)[\'"]', re.IGNORECASE),
            
            # Copy statements
            'copy_statement': re.compile(r'COPY\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            
            # SQL blocks
            'sql_block': re.compile(r'EXEC\s+SQL(.*?)END-EXEC', re.DOTALL | re.IGNORECASE),
            'sql_include': re.compile(r'EXEC\s+SQL\s+INCLUDE\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            
            # Data items - Basic pattern
            'data_item': re.compile(r'^\s*(\d+)\s+([A-Z][A-Z0-9-]*)', re.MULTILINE | re.IGNORECASE),
            
            # ENHANCED: Field usage patterns for LineageAnalyzer
            'move_to_field': re.compile(r'MOVE\s+([^.]+?)\s+TO\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            'move_from_field': re.compile(r'MOVE\s+([A-Z][A-Z0-9-]*)\s+TO\s+([^.]+)', re.IGNORECASE),
            'compute_field': re.compile(r'COMPUTE\s+([A-Z][A-Z0-9-]*)\s*=\s*([^.]+)', re.IGNORECASE),
            'if_field': re.compile(r'IF\s+([A-Z][A-Z0-9-]*)', re.IGNORECASE),
            'picture_clause': re.compile(r'PIC\s+([X9S\(\)V\-\+,]+)', re.IGNORECASE),
        }
        
        # CICS Patterns - Core commands only
        self.cics_patterns = {
            'cics_link': re.compile(r'EXEC\s+CICS\s+LINK\s+PROGRAM\s*\(\s*([A-Z][A-Z0-9-]*)\s*\)', re.IGNORECASE),
            'cics_xctl': re.compile(r'EXEC\s+CICS\s+XCTL\s+PROGRAM\s*\(\s*([A-Z][A-Z0-9-]*)\s*\)', re.IGNORECASE),
            'cics_read': re.compile(r'EXEC\s+CICS\s+READ\s+FILE\s*\(\s*([A-Z][A-Z0-9-]*)\s*\)', re.IGNORECASE),
            'cics_write': re.compile(r'EXEC\s+CICS\s+WRITE\s+FILE\s*\(\s*([A-Z][A-Z0-9-]*)\s*\)', re.IGNORECASE),
        }
        
        # JCL Patterns - Essential only
        self.jcl_patterns = {
            'job_card': re.compile(r'^//(\w+)\s+JOB\s', re.MULTILINE),
            'exec_pgm': re.compile(r'^//(\w+)\s+EXEC\s+PGM=([A-Z0-9]+)', re.MULTILINE | re.IGNORECASE),
            'exec_proc': re.compile(r'^//(\w+)\s+EXEC\s+PROC=([A-Z0-9]+)', re.MULTILINE | re.IGNORECASE),
            'exec_simple': re.compile(r'^//(\w+)\s+EXEC\s+([A-Z0-9]+)', re.MULTILINE | re.IGNORECASE),
            'dd_statement': re.compile(r'^//(\w+)\s+DD\s+DSN=([^,\s]+)', re.MULTILINE | re.IGNORECASE),
        }
        
        # MQ Patterns - API calls only
        self.mq_patterns = {
            'mq_call': re.compile(r'CALL\s+[\'"]MQ([A-Z]+)[\'"]', re.IGNORECASE),
            'mq_queue': re.compile(r'[\'"]([A-Z][A-Z0-9\._]*\.Q)[\'"]', re.IGNORECASE),
        }
        
        # DB2 Patterns - Basic SQL
        self.db2_patterns = {
            'create_procedure': re.compile(r'CREATE\s+(?:OR\s+REPLACE\s+)?PROCEDURE\s+([A-Z][A-Z0-9_]*)', re.IGNORECASE),
            'sql_table': re.compile(r'FROM\s+([A-Z][A-Z0-9_]*)', re.IGNORECASE),
            'sql_insert': re.compile(r'INSERT\s+INTO\s+([A-Z][A-Z0-9_]*)', re.IGNORECASE),
            'sql_update': re.compile(r'UPDATE\s+([A-Z][A-Z0-9_]*)', re.IGNORECASE),
        }

    def _init_enhanced_database_with_lineage(self):
        """FIXED: Initialize database schema with corrected SQL syntax"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ==================== EXISTING CODEPARSER TABLES ====================
            
            # Core chunks table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS program_chunks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    program_name TEXT NOT NULL,
                    chunk_id TEXT NOT NULL,
                    chunk_type TEXT NOT NULL,
                    content TEXT NOT NULL,
                    metadata TEXT,
                    file_hash TEXT,
                    confidence_score REAL DEFAULT 1.0,
                    line_start INTEGER,
                    line_end INTEGER,
                    created_timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(program_name, chunk_id)
                )
            """)
            
            # FIXED: Program relationships with all required columns
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS program_relationships (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    calling_program TEXT NOT NULL,
                    called_program TEXT NOT NULL,
                    call_type TEXT NOT NULL,
                    call_location TEXT,
                    line_number INTEGER,
                    call_statement TEXT,
                    parameters TEXT,
                    replacing_clause TEXT,
                    conditional_call INTEGER DEFAULT 0,
                    created_timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(calling_program, called_program, call_type, line_number)
                )
            """)
            
            # FIXED: File relationships with all required columns
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS file_access_relationships (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    program_name TEXT NOT NULL,
                    logical_file_name TEXT NOT NULL,
                    physical_file_name TEXT,
                    access_type TEXT NOT NULL,
                    access_mode TEXT,
                    line_number INTEGER,
                    access_statement TEXT,
                    record_format TEXT,
                    created_timestamp TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # FIXED: Copybook relationships with all required columns and proper syntax
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS copybook_relationships (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    program_name TEXT NOT NULL,
                    copybook_name TEXT NOT NULL,
                    copy_location TEXT,
                    line_number INTEGER,
                    copy_statement TEXT,
                    replacing_clause TEXT,
                    usage_context TEXT,
                    created_timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(program_name, copybook_name, copy_location)
                )
            """)
            
            # Field definitions - Simplified
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS field_definitions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    field_name TEXT NOT NULL,
                    source_name TEXT NOT NULL,
                    source_type TEXT NOT NULL,
                    level_number INTEGER,
                    data_type TEXT,
                    picture_clause TEXT,
                    line_number INTEGER,
                    created_timestamp TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # SQL analysis - Basic
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS sql_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    program_name TEXT NOT NULL,
                    sql_type TEXT NOT NULL,
                    tables_accessed TEXT,
                    operation_type TEXT,
                    line_number INTEGER,
                    sql_statement TEXT,
                    created_timestamp TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # LLM analysis cache
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS llm_analysis_cache (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    content_hash TEXT NOT NULL UNIQUE,
                    analysis_type TEXT NOT NULL,
                    analysis_result TEXT,
                    confidence_score REAL,
                    created_timestamp TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # File metadata (enhanced for LineageAnalyzer)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS file_metadata (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_name TEXT UNIQUE NOT NULL,
                    file_type TEXT,
                    table_name TEXT,
                    fields TEXT,
                    source_type TEXT,
                    last_modified TIMESTAMP,
                    processing_status TEXT DEFAULT 'pending',
                    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # ==================== NEW LINEAGE ANALYZER TABLES ====================
            
            # Lineage graph tables
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS lineage_nodes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    node_id TEXT UNIQUE,
                    node_type TEXT,
                    name TEXT,
                    properties TEXT,
                    source_location TEXT,
                    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS lineage_edges (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_node_id TEXT,
                    target_node_id TEXT,
                    relationship_type TEXT,
                    properties TEXT,
                    confidence_score REAL DEFAULT 1.0,
                    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (source_node_id) REFERENCES lineage_nodes (node_id),
                    FOREIGN KEY (target_node_id) REFERENCES lineage_nodes (node_id)
                )
            """)
            
            # Field tracking tables
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS field_usage_tracking (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    field_name TEXT,
                    program_name TEXT,
                    paragraph TEXT,
                    operation_type TEXT,
                    operation_context TEXT,
                    source_line INTEGER,
                    confidence_score REAL DEFAULT 1.0,
                    discovered_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS field_cross_reference (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    field_name TEXT,
                    qualified_name TEXT,
                    source_type TEXT,
                    source_name TEXT,
                    definition_location TEXT,
                    data_type TEXT,
                    picture_clause TEXT,
                    usage_clause TEXT,
                    level_number INTEGER,
                    parent_field TEXT,
                    occurs_info TEXT,
                    business_domain TEXT,
                    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Lifecycle and analysis tables
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS component_lifecycle (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    component_name TEXT,
                    component_type TEXT,
                    lifecycle_stage TEXT,
                    program_name TEXT,
                    job_name TEXT,
                    operation_details TEXT,
                    timestamp_info TEXT,
                    discovered_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS data_flow_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    flow_name TEXT,
                    source_component TEXT,
                    target_component TEXT,
                    transformation_logic TEXT,
                    business_rules TEXT,
                    data_quality_checks TEXT,
                    performance_characteristics TEXT,
                    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS partial_analysis_cache (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    component_name TEXT UNIQUE,
                    agent_type TEXT,
                    partial_data TEXT,
                    progress_percent REAL,
                    status TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # CRITICAL FIX: Impact analysis table (was duplicated and had missing columns)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS impact_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_artifact TEXT,
                    source_type TEXT,
                    dependent_artifact TEXT,
                    dependent_type TEXT,
                    relationship_type TEXT,
                    impact_level TEXT,
                    change_propagation TEXT,
                    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # ==================== CREATE INDEXES ====================
            
            # Existing indexes
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_prog_chunks_name ON program_chunks(program_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_prog_rel_calling ON program_relationships(calling_program)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_file_rel_program ON file_access_relationships(program_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_copy_rel_program ON copybook_relationships(program_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_field_def_source ON field_definitions(source_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_sql_program ON sql_analysis(program_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_file_metadata_name ON file_metadata(file_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_file_metadata_type ON file_metadata(file_type)")
            
            # New LineageAnalyzer indexes
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_lineage_nodes_type ON lineage_nodes(node_type)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_lineage_nodes_name ON lineage_nodes(name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_lineage_edges_source ON lineage_edges(source_node_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_lineage_edges_target ON lineage_edges(target_node_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_field_usage_field ON field_usage_tracking(field_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_field_usage_program ON field_usage_tracking(program_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_field_xref_field ON field_cross_reference(field_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_field_xref_source ON field_cross_reference(source_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_component_lifecycle_name ON component_lifecycle(component_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_component_lifecycle_type ON component_lifecycle(component_type)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_data_flow_source ON data_flow_analysis(source_component)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_data_flow_target ON data_flow_analysis(target_component)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_partial_cache_component ON partial_analysis_cache(component_name)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_impact_source ON impact_analysis(source_artifact)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_impact_dependent ON impact_analysis(dependent_artifact)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_impact_level ON impact_analysis(impact_level)")

            conn.commit()
            conn.close()
            
            self.logger.info("‚úÖ FIXED database schema initialized with corrected SQL syntax")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {str(e)}")
            raise

    # CRITICAL FIX: Add missing methods expected by coordinator
    def get_agent_stats(self) -> Dict[str, Any]:
        """Get agent statistics for coordinator"""
        return {
            "agent_type": self.agent_type,
            "gpu_id": self.gpu_id,
            "files_processed": self._files_processed,
            "total_chunks_created": self._total_chunks,
            "api_calls_made": self._api_calls,
            "successful_analyses": self._successful_analyses,
            "failed_analyses": self._failed_analyses,
            "success_rate": (
                (self._successful_analyses / max(1, self._successful_analyses + self._failed_analyses)) * 100
            ),
            "status": "ready",
            "api_based": True,
            "lineage_enhanced": True,
            "database_path": self.db_path,
            "context_window": self.max_context_tokens,
            "current_api_params": self.api_params.copy()
        }

    def update_api_params(self, **params):
        """CRITICAL FIX: Update API parameters from coordinator with validation"""
        for key, value in params.items():
            if key == 'max_tokens':
                # FIXED: Apply conservative limits
                self.api_params[key] = min(value, 30)  # Never exceed 30 tokens
            elif key == 'temperature':
                # FIXED: Keep temperature conservative
                self.api_params[key] = min(value, 0.15)  # Never exceed 0.15
            elif key == 'top_p':
                self.api_params[key] = min(value, 0.9)
            else:
                self.api_params[key] = value
        
        self.logger.info(f"‚úÖ Updated API params (with limits): {self.api_params}")

    async def process_file(self, file_path: Path) -> Dict[str, Any]:
        """ENHANCED: Main file processing method with LineageAnalyzer data population"""
        try:
            self.logger.info(f"üîç Processing file: {file_path}")
            self._files_processed += 1
            
            # Read file content
            content = await self._read_file_safely(file_path)
            if not content:
                return {"status": "error", "error": "Could not read file"}
            
            # Detect file type
            file_type = self._detect_file_type_simple(content, file_path.suffix)
            program_name = self._extract_program_name_simple(content, file_path)
            
            self.logger.info(f"üìã File type: {file_type}, Program: {program_name}")
            
            # Extract relationships FIRST (reliable patterns)
            relationships = await self._extract_all_relationships(content, program_name, file_type)
            
            # Store relationships
            await self._store_all_relationships(relationships)
            
            # Create basic chunks
            chunks = await self._create_basic_chunks(content, program_name, file_type)
            self._total_chunks += len(chunks)
            
            # ENHANCED: Extract and store lineage data
            await self._extract_and_store_lineage_data(content, program_name, file_type, chunks, relationships)
            
            # FIXED: Enhanced analysis using coordinator's API
            if len(content) > 100:  # Only for substantial content
                enhanced_analysis = await self._enhanced_llm_analysis_via_coordinator(
                    content, file_type, program_name
                )
                # Merge enhanced analysis into chunks
                chunks = self._merge_enhanced_analysis(chunks, enhanced_analysis)
            
            # Store chunks
            await self._store_chunks(chunks, file_path)
            
            # ENHANCED: Store file metadata for LineageAnalyzer
            await self._store_file_metadata(file_path, file_type, program_name, chunks)
            
            return {
                "status": "success",
                "file_name": str(file_path.name),
                "file_type": file_type,
                "program_name": program_name,
                "chunks_created": len(chunks),
                "relationships_found": len(relationships),
                "lineage_enhanced": True,
                "processing_timestamp": dt.now().isoformat(),
                "coordinator_api_used": True
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Processing failed for {file_path}: {str(e)}")
            self._failed_analyses += 1
            return {
                "status": "error",
                "file_name": str(file_path.name),
                "error": str(e)
            }

    async def _extract_and_store_lineage_data(self, content: str, program_name: str, 
                                            file_type: str, chunks: List[CodeChunk], 
                                            relationships: List[RelationshipRecord]):
        """ENHANCED: Extract and store comprehensive lineage data for LineageAnalyzer"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # 1. Extract and store field definitions and cross-references
            await self._extract_field_definitions(cursor, content, program_name, file_type)
            
            # 2. Extract and store field usage patterns
            await self._extract_field_usage_patterns(cursor, content, program_name, chunks)
            
            # 3. Create lineage nodes for this program/file
            await self._create_lineage_nodes(cursor, program_name, file_type, chunks)
            
            # 4. Create lineage edges from relationships
            await self._create_lineage_edges(cursor, relationships, program_name)
            
            # 5. Store component lifecycle information
            await self._store_component_lifecycle(cursor, program_name, file_type, content)
            
            # 6. Analyze and store data flows
            await self._analyze_and_store_data_flows(cursor, content, program_name, relationships)
            
            # 7. Store impact analysis data
            await self._store_impact_analysis_data(cursor, program_name, relationships)
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ Stored comprehensive lineage data for {program_name}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to store lineage data: {e}")

    async def _extract_field_definitions(self, cursor, content: str, program_name: str, file_type: str):
        """Extract field definitions for field_cross_reference table"""
        try:
            # Extract data items with picture clauses
            data_item_pattern = re.compile(
                r'^\s*(\d+)\s+([A-Z][A-Z0-9-]*)\s+PIC\s+([X9S\(\)V\-\+,]+)(?:\s+USAGE\s+([A-Z]+))?',
                re.MULTILINE | re.IGNORECASE
            )
            
            matches = data_item_pattern.finditer(content)
            for match in matches:
                level_number = int(match.group(1))
                field_name = match.group(2).strip()
                picture_clause = match.group(3).strip()
                usage_clause = match.group(4).strip() if match.group(4) else None
                
                # Determine definition location
                line_pos = content[:match.start()].count('\n') + 1
                section = self._find_section(content, match.start())
                
                # Determine data type from picture clause
                data_type = self._determine_data_type(picture_clause)
                
                # Determine business domain (simple heuristics)
                business_domain = self._infer_business_domain(field_name)
                
                cursor.execute("""
                    INSERT OR IGNORE INTO field_cross_reference 
                    (field_name, qualified_name, source_type, source_name, definition_location,
                     data_type, picture_clause, usage_clause, level_number, parent_field,
                     occurs_info, business_domain)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    field_name,
                    f"{program_name}.{field_name}",
                    file_type,
                    program_name,
                    section,
                    data_type,
                    picture_clause,
                    usage_clause,
                    level_number,
                    None,  # parent_field - would need hierarchy analysis
                    json.dumps({}),  # occurs_info - would need OCCURS parsing
                    business_domain
                ))
            
            self.logger.debug(f"Extracted field definitions for {program_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to extract field definitions: {e}")

    def _determine_data_type(self, picture_clause: str) -> str:
        """Determine data type from COBOL picture clause"""
        pic = picture_clause.upper()
        
        if 'X' in pic:
            return "CHARACTER"
        elif '9' in pic and 'V' in pic:
            return "DECIMAL"
        elif '9' in pic:
            return "NUMERIC"
        elif 'S' in pic:
            return "SIGNED_NUMERIC"
        else:
            return "UNKNOWN"

    def _infer_business_domain(self, field_name: str) -> str:
        """Infer business domain from field name"""
        field_upper = field_name.upper()
        
        # Customer related
        if any(keyword in field_upper for keyword in ['CUST', 'CUSTOMER', 'CLIENT']):
            return "CUSTOMER"
        # Financial
        elif any(keyword in field_upper for keyword in ['AMT', 'AMOUNT', 'BAL', 'BALANCE', 'RATE', 'FEE']):
            return "FINANCIAL"
        # Account related
        elif any(keyword in field_upper for keyword in ['ACCT', 'ACCOUNT', 'ACC']):
            return "ACCOUNT"
        # Date/Time
        elif any(keyword in field_upper for keyword in ['DATE', 'TIME', 'TIMESTAMP', 'DT']):
            return "TEMPORAL"
        # Transaction
        elif any(keyword in field_upper for keyword in ['TXN', 'TRANS', 'TRANSACTION']):
            return "TRANSACTION"
        # Address
        elif any(keyword in field_upper for keyword in ['ADDR', 'ADDRESS', 'STREET', 'CITY', 'STATE', 'ZIP']):
            return "ADDRESS"
        else:
            return "GENERAL"

    async def _extract_field_usage_patterns(self, cursor, content: str, program_name: str, chunks: List[CodeChunk]):
        """Extract field usage patterns for field_usage_tracking table"""
        try:
            for chunk in chunks:
                if chunk.chunk_type in ['cobol_procedure_division', 'procedure_division']:
                    chunk_content = chunk.content
                    
                    # Find MOVE operations
                    move_matches = self.cobol_patterns['move_to_field'].finditer(chunk_content)
                    for match in move_matches:
                        source = match.group(1).strip()
                        target = match.group(2).strip()
                        line_num = chunk_content[:match.start()].count('\n') + chunk.line_start
                        
                        cursor.execute("""
                            INSERT INTO field_usage_tracking 
                            (field_name, program_name, paragraph, operation_type, operation_context, source_line)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            target,
                            program_name,
                            self._find_paragraph(chunk_content, match.start()),
                            "WRITE",
                            f"MOVE {source} TO {target}",
                            line_num
                        ))
                    
                    # Find MOVE from field operations
                    move_from_matches = self.cobol_patterns['move_from_field'].finditer(chunk_content)
                    for match in move_from_matches:
                        source = match.group(1).strip()
                        target = match.group(2).strip()
                        line_num = chunk_content[:match.start()].count('\n') + chunk.line_start
                        
                        cursor.execute("""
                            INSERT INTO field_usage_tracking 
                            (field_name, program_name, paragraph, operation_type, operation_context, source_line)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            source,
                            program_name,
                            self._find_paragraph(chunk_content, match.start()),
                            "READ",
                            f"MOVE {source} TO {target}",
                            line_num
                        ))
                    
                    # Find COMPUTE operations
                    compute_matches = self.cobol_patterns['compute_field'].finditer(chunk_content)
                    for match in compute_matches:
                        field = match.group(1).strip()
                        expression = match.group(2).strip()
                        line_num = chunk_content[:match.start()].count('\n') + chunk.line_start
                        
                        cursor.execute("""
                            INSERT INTO field_usage_tracking 
                            (field_name, program_name, paragraph, operation_type, operation_context, source_line)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            field,
                            program_name,
                            self._find_paragraph(chunk_content, match.start()),
                            "TRANSFORM",
                            f"COMPUTE {field} = {expression}",
                            line_num
                        ))
                    
                    # Find IF conditions (field validation)
                    if_matches = self.cobol_patterns['if_field'].finditer(chunk_content)
                    for match in if_matches:
                        field = match.group(1).strip()
                        line_num = chunk_content[:match.start()].count('\n') + chunk.line_start
                        
                        cursor.execute("""
                            INSERT INTO field_usage_tracking 
                            (field_name, program_name, paragraph, operation_type, operation_context, source_line)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            field,
                            program_name,
                            self._find_paragraph(chunk_content, match.start()),
                            "VALIDATE",
                            f"IF {field} condition",
                            line_num
                        ))
            
            self.logger.debug(f"Extracted field usage patterns for {program_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to extract field usage patterns: {e}")

    async def _create_lineage_nodes(self, cursor, program_name: str, file_type: str, chunks: List[CodeChunk]):
        """Create lineage nodes for lineage_nodes table"""
        try:
            # Create program node
            program_node_id = f"program_{program_name}"
            cursor.execute("""
                INSERT OR IGNORE INTO lineage_nodes 
                (node_id, node_type, name, properties, source_location)
                VALUES (?, ?, ?, ?, ?)
            """, (
                program_node_id,
                "program",
                program_name,
                json.dumps({
                    "file_type": file_type,
                    "chunk_count": len(chunks),
                    "agent": "code_parser"
                }),
                file_type
            ))
            
            # Create chunk nodes
            for chunk in chunks:
                chunk_node_id = f"chunk_{chunk.chunk_id}"
                cursor.execute("""
                    INSERT OR IGNORE INTO lineage_nodes 
                    (node_id, node_type, name, properties, source_location)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    chunk_node_id,
                    "chunk",
                    chunk.chunk_id,
                    json.dumps({
                        "chunk_type": chunk.chunk_type,
                        "program_name": chunk.program_name,
                        "line_range": f"{chunk.line_start}-{chunk.line_end}",
                        "confidence": chunk.confidence_score
                    }),
                    chunk.chunk_type
                ))
                
                # Create edge from program to chunk
                cursor.execute("""
                    INSERT OR IGNORE INTO lineage_edges 
                    (source_node_id, target_node_id, relationship_type, properties, confidence_score)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    program_node_id,
                    chunk_node_id,
                    "contains",
                    json.dumps({"relationship": "program_contains_chunk"}),
                    1.0
                ))
            
            self.logger.debug(f"Created lineage nodes for {program_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to create lineage nodes: {e}")

    async def _create_lineage_edges(self, cursor, relationships: List[RelationshipRecord], program_name: str):
        """Create lineage edges from relationships for lineage_edges table"""
        try:
            for rel in relationships:
                source_node_id = f"program_{rel.source_name}"
                target_node_id = f"program_{rel.target_name}"
                
                # Create nodes if they don't exist
                for node_id, node_name in [(source_node_id, rel.source_name), (target_node_id, rel.target_name)]:
                    cursor.execute("""
                        INSERT OR IGNORE INTO lineage_nodes 
                        (node_id, node_type, name, properties)
                        VALUES (?, ?, ?, ?)
                    """, (
                        node_id,
                        "program",
                        node_name,
                        json.dumps({"inferred": True})
                    ))
                
                # Create edge
                cursor.execute("""
                    INSERT OR IGNORE INTO lineage_edges 
                    (source_node_id, target_node_id, relationship_type, properties, confidence_score)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    source_node_id,
                    target_node_id,
                    rel.relationship_type,
                    json.dumps({
                        "location": rel.location,
                        "line_number": rel.line_number,
                        "statement": rel.metadata.get("statement", "")
                    }),
                    1.0
                ))
            
            self.logger.debug(f"Created lineage edges for {program_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to create lineage edges: {e}")

    async def _store_component_lifecycle(self, cursor, program_name: str, file_type: str, content: str):
        """Store component lifecycle information"""
        try:
            # Determine lifecycle stage based on content analysis
            if "IDENTIFICATION DIVISION" in content.upper():
                lifecycle_stage = "ACTIVE"
            elif "END PROGRAM" in content.upper():
                lifecycle_stage = "COMPLETE"
            else:
                lifecycle_stage = "PROCESSING"
            
            cursor.execute("""
                INSERT OR IGNORE INTO component_lifecycle 
                (component_name, component_type, lifecycle_stage, program_name, operation_details)
                VALUES (?, ?, ?, ?, ?)
            """, (
                program_name,
                file_type,
                lifecycle_stage,
                program_name,
                json.dumps({
                    "processing_timestamp": dt.now().isoformat(),
                    "content_length": len(content),
                    "analysis_agent": "code_parser"
                })
            ))
            
            self.logger.debug(f"Stored component lifecycle for {program_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to store component lifecycle: {e}")

    async def _analyze_and_store_data_flows(self, cursor, content: str, program_name: str, 
                                          relationships: List[RelationshipRecord]):
        """Analyze and store data flow information"""
        try:
            # Find file relationships for data flow analysis
            file_relationships = [r for r in relationships if r.relationship_type.startswith('FILE_')]
            
            for rel in file_relationships:
                # Determine flow direction
                if rel.relationship_type in ['FILE_READ', 'FILE_OPEN']:
                    source_component = rel.target_name  # File is source
                    target_component = program_name     # Program is target
                    flow_name = f"{source_component}_to_{target_component}"
                else:  # WRITE operations
                    source_component = program_name     # Program is source
                    target_component = rel.target_name  # File is target
                    flow_name = f"{source_component}_to_{target_component}"
                
                cursor.execute("""
                    INSERT OR IGNORE INTO data_flow_analysis 
                    (flow_name, source_component, target_component, transformation_logic, business_rules)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    flow_name,
                    source_component,
                    target_component,
                    f"File operation: {rel.relationship_type}",
                    f"Location: {rel.location}, Line: {rel.line_number}"
                ))
            
            self.logger.debug(f"Stored data flow analysis for {program_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to store data flow analysis: {e}")

    async def _store_impact_analysis_data(self, cursor, program_name: str, relationships: List[RelationshipRecord]):
        """Store impact analysis data for cross-program lineage"""
        try:
            for rel in relationships:
                # Determine impact level based on relationship type
                if rel.relationship_type in ['COBOL_CALL', 'CICS_LINK']:
                    impact_level = "HIGH"
                elif rel.relationship_type.startswith('FILE_'):
                    impact_level = "MEDIUM"
                else:
                    impact_level = "LOW"
                
                cursor.execute("""
                    INSERT OR IGNORE INTO impact_analysis 
                    (source_artifact, source_type, dependent_artifact, dependent_type,
                     relationship_type, impact_level, change_propagation)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    rel.source_name,
                    "program",
                    rel.target_name,
                    "program" if rel.relationship_type in ['COBOL_CALL', 'CICS_LINK'] else "file",
                    rel.relationship_type,
                    impact_level,
                    f"Changes to {rel.source_name} may impact {rel.target_name}"
                ))
            
            self.logger.debug(f"Stored impact analysis data for {program_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to store impact analysis data: {e}")

    async def _store_file_metadata(self, file_path: Path, file_type: str, program_name: str, chunks: List[CodeChunk]):
        """Store file metadata for LineageAnalyzer compatibility"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Extract field names from chunks
            field_names = []
            for chunk in chunks:
                # Extract fields from data division chunks
                if 'data' in chunk.chunk_type.lower():
                    data_items = self.cobol_patterns['data_item'].findall(chunk.content)
                    field_names.extend([item[1] for item in data_items])
            
            cursor.execute("""
                INSERT OR REPLACE INTO file_metadata 
                (file_name, file_type, table_name, fields, source_type, processing_status, last_modified)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                file_path.name,
                file_type,
                program_name,
                json.dumps(field_names),
                file_type,
                "processed",
                dt.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            self.logger.debug(f"Stored file metadata for {file_path.name}")
            
        except Exception as e:
            self.logger.error(f"Failed to store file metadata: {e}")

    # Keep all existing methods from the previous CodeParser implementation
    # ... (All the existing methods remain unchanged)

    async def _enhanced_llm_analysis_via_coordinator(self, content: str, file_type: str, 
                                                   program_name: str) -> Dict[str, Any]:
        """CRITICAL FIX: Enhanced analysis using coordinator's API instead of direct calls"""
        
        if not hasattr(self, 'coordinator') or not self.coordinator:
            self.logger.warning("‚ö†Ô∏è No coordinator available, using fallback analysis")
            return {"error": "No coordinator available", "fallback": True}
        
        # Estimate token count for coordinator
        estimated_tokens = len(content) // 4
        
        if estimated_tokens <= self.max_content_tokens:
            # Content fits in one request
            return await self._single_coordinator_analysis(content, file_type, program_name)
        else:
            # Need to chunk the content
            return await self._chunked_coordinator_analysis(content, file_type, program_name)

    async def _single_coordinator_analysis(self, content: str, file_type: str, 
                                         program_name: str) -> Dict[str, Any]:
        """FIXED: Single analysis call via coordinator"""
        
        # Check cache first
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        cached_result = await self._check_llm_cache(content_hash, file_type)
        if cached_result:
            return cached_result
        
        # Build analysis prompt
        prompt = self._build_analysis_prompt(content, file_type, program_name)
        
        try:
            self._api_calls += 1
            
            # CRITICAL FIX: Call coordinator's API with conservative parameters
            response = await self.coordinator.call_model_api(
                prompt=prompt,
                params={
                    "max_tokens": min(self.api_params.get("max_tokens", 20), 25),  # Very conservative
                    "temperature": self.api_params.get("temperature", 0.1),
                    "top_p": self.api_params.get("top_p", 0.9)
                }
            )
            
            # FIXED: Parse coordinator response
            analysis = self._parse_coordinator_response(response, file_type)
            
            # Cache result
            await self._cache_llm_result(content_hash, file_type, analysis)
            
            self._successful_analyses += 1
            return analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå Coordinator API analysis failed: {str(e)}")
            self._failed_analyses += 1
            return {"error": str(e), "fallback": True}

    async def _chunked_coordinator_analysis(self, content: str, file_type: str, program_name: str) -> Dict[str, Any]:
        """FIXED: Chunked analysis for large content"""
        try:
            # Simple chunking strategy
            chunk_size = self.max_content_tokens * 3  # Approximate chars per token
            chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
            
            analyses = []
            for i, chunk in enumerate(chunks[:3]):  # Limit to 3 chunks max
                chunk_analysis = await self._single_coordinator_analysis(
                    chunk, f"{file_type}_chunk_{i}", f"{program_name}_part_{i}"
                )
                if not chunk_analysis.get('error'):
                    analyses.append(chunk_analysis)
            
            # Aggregate results
            if analyses:
                return {
                    "aggregated": True,
                    "chunks_analyzed": len(analyses),
                    "confidence": sum(a.get('confidence', 0.5) for a in analyses) / len(analyses),
                    "complexity": max((a.get('complexity', 'low') for a in analyses), 
                                    key=lambda x: ['low', 'medium', 'high'].index(x) if x in ['low', 'medium', 'high'] else 0)
                }
            else:
                return {"error": "All chunks failed analysis", "fallback": True}
                
        except Exception as e:
            self.logger.error(f"‚ùå Chunked analysis failed: {str(e)}")
            return {"error": str(e), "fallback": True}

    # ==================== Additional Helper Methods ====================

    async def _read_file_safely(self, file_path: Path) -> Optional[str]:
        """Safely read file with multiple encoding attempts"""
        encodings = ['utf-8', 'cp1252', 'latin1', 'ascii']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                    return f.read()
            except Exception:
                continue
        
        return None

    def _detect_file_type_simple(self, content: str, suffix: str) -> str:
        """Simplified file type detection"""
        content_upper = content.upper()
        
        # Check content patterns first
        if 'CREATE PROCEDURE' in content_upper or 'CREATE OR REPLACE PROCEDURE' in content_upper:
            return 'db2_procedure'
        elif 'DFHMSD' in content_upper or 'DFHMDI' in content_upper:
            return 'bms'
        elif content.strip().startswith('//') and ('JOB' in content_upper or 'EXEC' in content_upper):
            return 'jcl'
        elif 'IDENTIFICATION DIVISION' in content_upper or 'PROGRAM-ID' in content_upper:
            if 'EXEC CICS' in content_upper:
                return 'cics'
            elif 'EXEC SQL' in content_upper and 'CALL' in content_upper and 'MQ' in content_upper:
                return 'cobol_stored_procedure'
            elif 'CALL' in content_upper and any(mq in content_upper for mq in ['MQOPEN', 'MQPUT', 'MQGET']):
                return 'mq_program'
            else:
                return 'cobol'
        elif 'PIC' in content_upper and not 'IDENTIFICATION DIVISION' in content_upper:
            return 'copybook'
        
        # Fallback to extension
        suffix_map = {
            '.cbl': 'cobol', '.cob': 'cobol',
            '.jcl': 'jcl',
            '.cpy': 'copybook', '.copy': 'copybook',
            '.bms': 'bms',
            '.sql': 'db2_procedure', '.db2': 'db2_procedure'
        }
        
        return suffix_map.get(suffix.lower(), 'unknown')

    def _extract_program_name_simple(self, content: str, file_path: Path) -> str:
        """Extract program name reliably"""
        # Try COBOL PROGRAM-ID
        if match := self.cobol_patterns['program_id'].search(content):
            return match.group(1).strip()
        
        # Try JCL job name
        if match := self.jcl_patterns['job_card'].search(content):
            return match.group(1).strip()
        
        # Try DB2 procedure
        if match := self.db2_patterns['create_procedure'].search(content):
            name = match.group(1).strip()
            return name.split('.')[-1] if '.' in name else name
        
        # Use filename
        return file_path.stem

    async def _extract_all_relationships(self, content: str, program_name: str, file_type: str) -> List[RelationshipRecord]:
        """Extract all relationships using reliable patterns"""
        relationships = []
        
        # Program calls
        relationships.extend(self._extract_program_calls(content, program_name))
        
        # File relationships
        relationships.extend(self._extract_file_relationships(content, program_name))
        
        # Copybook relationships
        relationships.extend(self._extract_copybook_relationships(content, program_name))
        
        # SQL relationships
        relationships.extend(self._extract_sql_relationships(content, program_name))
        
        self.logger.info(f"üìä Extracted {len(relationships)} relationships")
        return relationships

    def _extract_program_calls(self, content: str, program_name: str) -> List[RelationshipRecord]:
        """Extract program call relationships"""
        relationships = []
        
        # COBOL CALL statements
        for match in self.cobol_patterns['cobol_call'].finditer(content):
            called_program = match.group(1).strip()
            line_num = content[:match.start()].count('\n') + 1
            
            relationships.append(RelationshipRecord(
                source_name=program_name,
                target_name=called_program,
                relationship_type='COBOL_CALL',
                location=self._find_paragraph(content, match.start()),
                line_number=line_num,
                metadata={'statement': match.group(0)}
            ))
        
        # CICS LINK/XCTL
        for pattern_name, pattern in self.cics_patterns.items():
            if 'link' in pattern_name or 'xctl' in pattern_name:
                for match in pattern.finditer(content):
                    called_program = match.group(1).strip()
                    line_num = content[:match.start()].count('\n') + 1
                    
                    relationships.append(RelationshipRecord(
                        source_name=program_name,
                        target_name=called_program,
                        relationship_type=pattern_name.upper(),
                        location=self._find_paragraph(content, match.start()),
                        line_number=line_num,
                        metadata={'statement': match.group(0)}
                    ))
        
        # JCL EXEC statements
        for pattern_name, pattern in self.jcl_patterns.items():
            if 'exec' in pattern_name:
                for match in pattern.finditer(content):
                    step_name = match.group(1).strip()
                    program_name_exec = match.group(2).strip()
                    line_num = content[:match.start()].count('\n') + 1
                    
                    relationships.append(RelationshipRecord(
                        source_name=program_name,
                        target_name=program_name_exec,
                        relationship_type='JCL_EXEC',
                        location=step_name,
                        line_number=line_num,
                        metadata={'statement': match.group(0)}
                    ))
        
        return relationships

    def _extract_file_relationships(self, content: str, program_name: str) -> List[RelationshipRecord]:
        """Extract file access relationships"""
        relationships = []
        
        # File assignments (SELECT ... ASSIGN TO ...)
        for match in self.cobol_patterns['select_assign'].finditer(content):
            logical_file = match.group(1).strip()
            physical_file = match.group(2).strip()
            line_num = content[:match.start()].count('\n') + 1
            
            relationships.append(RelationshipRecord(
                source_name=program_name,
                target_name=logical_file,
                relationship_type='FILE_SELECT',
                location='FILE-CONTROL',
                line_number=line_num,
                metadata={
                    'logical_file': logical_file,
                    'physical_file': physical_file,
                    'statement': match.group(0)
                }
            ))
        
        # File operations
        file_ops = {
            'file_open': 'OPEN',
            'file_read': 'READ', 
            'file_write': 'WRITE',
            'file_close': 'CLOSE'
        }
        
        for pattern_name, op_type in file_ops.items():
            pattern = self.cobol_patterns[pattern_name]
            for match in pattern.finditer(content):
                if pattern_name == 'file_open':
                    file_mode = match.group(1).strip()
                    file_name = match.group(2).strip()
                else:
                    file_name = match.group(1).strip()
                    file_mode = ''
                
                line_num = content[:match.start()].count('\n') + 1
                
                relationships.append(RelationshipRecord(
                    source_name=program_name,
                    target_name=file_name,
                    relationship_type=f'FILE_{op_type}',
                    location=self._find_paragraph(content, match.start()),
                    line_number=line_num,
                    metadata={
                        'access_mode': file_mode,
                        'statement': match.group(0)
                    }
                ))
        
        return relationships

    def _extract_copybook_relationships(self, content: str, program_name: str) -> List[RelationshipRecord]:
        """Extract copybook relationships"""
        relationships = []
        
        for match in self.cobol_patterns['copy_statement'].finditer(content):
            copybook_name = match.group(1).strip()
            line_num = content[:match.start()].count('\n') + 1
            
            relationships.append(RelationshipRecord(
                source_name=program_name,
                target_name=copybook_name,
                relationship_type='COPY',
                location=self._find_section(content, match.start()),
                line_number=line_num,
                metadata={'statement': match.group(0)}
            ))
        
        return relationships

    def _extract_sql_relationships(self, content: str, program_name: str) -> List[RelationshipRecord]:
        """Extract SQL table relationships"""
        relationships = []
        
        # Extract tables from SQL blocks
        for sql_match in self.cobol_patterns['sql_block'].finditer(content):
            sql_content = sql_match.group(1)
            line_num = content[:sql_match.start()].count('\n') + 1
            
            # Find tables in this SQL block
            for pattern_name, pattern in self.db2_patterns.items():
                for table_match in pattern.finditer(sql_content):
                    table_name = table_match.group(1).strip()
                    
                    relationships.append(RelationshipRecord(
                        source_name=program_name,
                        target_name=table_name,
                        relationship_type=f'SQL_{pattern_name.upper()}',
                        location=self._find_paragraph(content, sql_match.start()),
                        line_number=line_num,
                        metadata={
                            'sql_statement': sql_content[:100],
                            'operation': pattern_name
                        }
                    ))
        
        return relationships

    def _find_paragraph(self, content: str, position: int) -> str:
        """Find containing paragraph"""
        before_content = content[:position]
        lines = before_content.split('\n')
        
        for line in reversed(lines):
            line = line.strip()
            if line and not line.startswith('*') and line.endswith('.'):
                # Simple paragraph detection
                words = line.split()
                if len(words) >= 1 and words[0].replace('-', '').isalnum():
                    return words[0]
        
        return 'UNKNOWN'

    def _find_section(self, content: str, position: int) -> str:
        """Find containing section"""
        before_content = content[:position].upper()
        
        sections = [
            'WORKING-STORAGE SECTION',
            'FILE SECTION', 
            'LINKAGE SECTION',
            'PROCEDURE DIVISION'
        ]
        
        current_section = 'UNKNOWN'
        for section in sections:
            if section in before_content:
                current_section = section.replace(' SECTION', '').replace(' ', '-')
        
        return current_section

    async def _store_all_relationships(self, relationships: List[RelationshipRecord]):
        """FIXED: Store all relationships in appropriate tables with proper column names"""
        if not relationships:
            return
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for rel in relationships:
                if rel.relationship_type in ['COBOL_CALL', 'CICS_LINK', 'CICS_XCTL', 'JCL_EXEC']:
                    # Program relationships - FIXED: Include replacing_clause column
                    cursor.execute("""
                        INSERT OR IGNORE INTO program_relationships 
                        (calling_program, called_program, call_type, call_location, line_number, call_statement, replacing_clause)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        rel.source_name, rel.target_name, rel.relationship_type,
                        rel.location, rel.line_number, rel.metadata.get('statement', ''), ''
                    ))
                
                elif rel.relationship_type.startswith('FILE_') or rel.relationship_type.startswith('CICS_READ') or rel.relationship_type.startswith('CICS_WRITE'):
                    # File relationships
                    cursor.execute("""
                        INSERT OR IGNORE INTO file_access_relationships 
                        (program_name, logical_file_name, physical_file_name, access_type, access_mode, line_number, access_statement)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        rel.source_name, rel.target_name, 
                        rel.metadata.get('physical_file', ''),
                        rel.relationship_type,
                        rel.metadata.get('access_mode', ''),
                        rel.line_number, rel.metadata.get('statement', '')
                    ))
                
                elif rel.relationship_type == 'COPY':
                    # Copybook relationships - FIXED: Include replacing_clause and usage_context columns
                    cursor.execute("""
                        INSERT OR IGNORE INTO copybook_relationships 
                        (program_name, copybook_name, copy_location, line_number, copy_statement, replacing_clause, usage_context)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        rel.source_name, rel.target_name, rel.location,
                        rel.line_number, rel.metadata.get('statement', ''), '', 'DATA_STRUCTURE'
                    ))
                
                elif rel.relationship_type.startswith('SQL_'):
                    # SQL relationships
                    cursor.execute("""
                        INSERT OR IGNORE INTO sql_analysis 
                        (program_name, sql_type, tables_accessed, operation_type, line_number, sql_statement)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        rel.source_name, 'embedded_sql', rel.target_name,
                        rel.relationship_type, rel.line_number,
                        rel.metadata.get('sql_statement', '')
                    ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ Stored {len(relationships)} relationships")
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to store relationships: {str(e)}")

    async def _create_basic_chunks(self, content: str, program_name: str, file_type: str) -> List[CodeChunk]:
        """Create basic chunks for the content"""
        chunks = []
        
        # Create file-level chunk
        chunks.append(CodeChunk(
            program_name=program_name,
            chunk_id=f"{program_name}_MAIN",
            chunk_type=f"{file_type}_main",
            content=content[:min(1000, len(content))],  # First 1000 chars
            metadata={
                'file_type': file_type,
                'total_lines': content.count('\n') + 1,
                'content_size': len(content)
            },
            line_start=1,
            line_end=min(50, content.count('\n') + 1)
        ))
        
        # Create division/section chunks for COBOL
        if file_type in ['cobol', 'cics', 'cobol_stored_procedure']:
            chunks.extend(self._create_cobol_chunks(content, program_name))
        
        # Create JCL step chunks
        elif file_type == 'jcl':
            chunks.extend(self._create_jcl_chunks(content, program_name))
        
        # Create copybook chunks
        elif file_type == 'copybook':
            chunks.extend(self._create_copybook_chunks(content, program_name))
        
        return chunks

    def _create_cobol_chunks(self, content: str, program_name: str) -> List[CodeChunk]:
        """Create COBOL-specific chunks"""
        chunks = []
        
        # Find divisions
        divisions = [
            ('IDENTIFICATION DIVISION', 'identification'),
            ('ENVIRONMENT DIVISION', 'environment'),
            ('DATA DIVISION', 'data'),
            ('PROCEDURE DIVISION', 'procedure')
        ]
        
        for div_pattern, div_name in divisions:
            if div_pattern in content.upper():
                start_pos = content.upper().find(div_pattern)
                # Find next division or end
                end_pos = len(content)
                for next_div, _ in divisions:
                    next_pos = content.upper().find(next_div, start_pos + 1)
                    if next_pos != -1 and next_pos < end_pos:
                        end_pos = next_pos
                
                division_content = content[start_pos:end_pos]
                if len(division_content.strip()) > 10:
                    chunks.append(CodeChunk(
                        program_name=program_name,
                        chunk_id=f"{program_name}_{div_name.upper()}_DIV",
                        chunk_type=f"cobol_{div_name}_division",
                        content=division_content,
                        metadata={
                            'division_name': div_name,
                            'division_size': len(division_content.split('\n'))
                        },
                        line_start=content[:start_pos].count('\n') + 1,
                        line_end=content[:end_pos].count('\n') + 1
                    ))
        
        return chunks

    def _create_jcl_chunks(self, content: str, program_name: str) -> List[CodeChunk]:
        """Create JCL-specific chunks"""
        chunks = []
        
        # Find job steps
        for match in self.jcl_patterns['exec_pgm'].finditer(content):
            step_name = match.group(1)
            pgm_name = match.group(2)
            
            # Get step content (from this EXEC to next // or end)
            start_pos = match.start()
            next_step = content.find('\n//', start_pos + 1)
            end_pos = next_step if next_step != -1 else len(content)
            
            step_content = content[start_pos:end_pos]
            
            chunks.append(CodeChunk(
                program_name=program_name,
                chunk_id=f"{program_name}_STEP_{step_name}",
                chunk_type="jcl_step",
                content=step_content,
                metadata={
                    'step_name': step_name,
                    'program_executed': pgm_name
                },
                line_start=content[:start_pos].count('\n') + 1,
                line_end=content[:end_pos].count('\n') + 1
            ))
        
        return chunks

    def _create_copybook_chunks(self, content: str, program_name: str) -> List[CodeChunk]:
        """Create copybook-specific chunks"""
        chunks = []
        
        # Find 01 level items (record definitions)
        record_pattern = re.compile(r'^\s*01\s+([A-Z][A-Z0-9-]*)', re.MULTILINE | re.IGNORECASE)
        matches = list(record_pattern.finditer(content))
        
        for i, match in enumerate(matches):
            record_name = match.group(1)
            start_pos = match.start()
            
            # Find end (next 01 level or end of content)
            if i + 1 < len(matches):
                end_pos = matches[i + 1].start()
            else:
                end_pos = len(content)
            
            record_content = content[start_pos:end_pos]
            
            chunks.append(CodeChunk(
                program_name=program_name,
                chunk_id=f"{program_name}_RECORD_{record_name}",
                chunk_type="copybook_record",
                content=record_content,
                metadata={
                    'record_name': record_name,
                    'field_count': record_content.count('PIC')
                },
                line_start=content[:start_pos].count('\n') + 1,
                line_end=content[:end_pos].count('\n') + 1
            ))
        
        return chunks

    def _parse_coordinator_response(self, response: Dict[str, Any], file_type: str) -> Dict[str, Any]:
        """FIXED: Parse response from coordinator API call"""
        try:
            # Extract text from coordinator response
            text_content = (
                response.get('text') or 
                response.get('response') or 
                response.get('content') or
                response.get('generated_text') or
                str(response.get('choices', [{}])[0].get('text', ''))
            )
            
            if not text_content:
                return {"error": "No text content in coordinator response", "fallback": True}
            
            # Try to parse as JSON first
            if '{' in text_content and '}' in text_content:
                start = text_content.find('{')
                end = text_content.rfind('}') + 1
                json_str = text_content[start:end]
                
                try:
                    analysis = json.loads(json_str)
                    analysis['coordinator_analysis'] = True
                    analysis['confidence'] = 0.8
                    return analysis
                except json.JSONDecodeError:
                    pass
            
            # Fallback parsing
            return self._fallback_parse_coordinator_response(text_content, file_type)
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to parse coordinator response: {e}")
            return {"error": str(e), "fallback": True}

    def _fallback_parse_coordinator_response(self, text_content: str, file_type: str) -> Dict[str, Any]:
        """FIXED: Fallback parsing for coordinator responses"""
        analysis = {
            'coordinator_analysis': True,
            'confidence': 0.6,
            'raw_response': text_content[:300],  # First 300 chars
            'fallback_parsing': True
        }
        
        # Extract key information using simple patterns
        text_lower = text_content.lower()
        
        if any(word in text_lower for word in ['complex', 'complicated', 'difficult']):
            analysis['complexity'] = 'high'
        elif any(word in text_lower for word in ['simple', 'basic', 'straightforward']):
            analysis['complexity'] = 'low'
        else:
            analysis['complexity'] = 'medium'
        
        # Extract business purpose based on file type
        if file_type == 'cobol':
            analysis['business_purpose'] = 'data_processing_program'
        elif file_type == 'jcl':
            analysis['job_purpose'] = 'batch_job_execution'
        elif file_type == 'copybook':
            analysis['data_purpose'] = 'data_structure_definition'
        
        return analysis

    def _build_analysis_prompt(self, content: str, file_type: str, program_name: str) -> str:
        """Build analysis prompt based on file type with conservative token usage"""
        
        # FIXED: Much shorter prompts for coordinator's token limits
        base_prompts = {
            'cobol': f"""Analyze COBOL program '{program_name}':

{content[:800]}

JSON format:
{{"business_purpose": "brief description", "complexity": "low|medium|high", "key_operations": ["op1", "op2"]}}""",
            
            'jcl': f"""Analyze JCL job '{program_name}':

{content[:800]}

JSON format:
{{"job_purpose": "brief description", "complexity": "low|medium|high", "steps": ["step1", "step2"]}}""",
            
            'copybook': f"""Analyze copybook '{program_name}':

{content[:800]}

JSON format:
{{"data_purpose": "brief description", "complexity": "low|medium|high", "record_types": ["type1"]}}""",
            
            'cics': f"""Analyze CICS program '{program_name}':

{content[:800]}

JSON format:
{{"transaction_purpose": "brief description", "complexity": "low|medium|high", "cics_resources": ["map", "file"]}}"""
        }
        
        return base_prompts.get(file_type, base_prompts['cobol'])

    async def _check_llm_cache(self, content_hash: str, analysis_type: str) -> Optional[Dict[str, Any]]:
        """Check LLM analysis cache"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT analysis_result, confidence_score 
                FROM llm_analysis_cache 
                WHERE content_hash = ? AND analysis_type = ?
            """, (content_hash, analysis_type))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                self.logger.info(f"üìã Cache HIT for {analysis_type}")
                return {
                    'analysis': json.loads(result[0]),
                    'confidence': result[1],
                    'cached': True
                }
            
            return None
            
        except Exception as e:
            self.logger.warning(f"Cache check failed: {str(e)}")
            return None

    async def _cache_llm_result(self, content_hash: str, analysis_type: str, analysis: Dict[str, Any]):
        """Cache LLM analysis result"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR REPLACE INTO llm_analysis_cache 
                (content_hash, analysis_type, analysis_result, confidence_score)
                VALUES (?, ?, ?, ?)
            """, (
                content_hash, analysis_type, 
                json.dumps(analysis), analysis.get('confidence', 0.5)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.warning(f"Cache storage failed: {str(e)}")

    def _merge_enhanced_analysis(self, chunks: List[CodeChunk], enhanced_analysis: Dict[str, Any]) -> List[CodeChunk]:
        """Merge enhanced coordinator analysis into chunks"""
        
        if enhanced_analysis.get('error') or enhanced_analysis.get('fallback'):
            return chunks  # Return chunks as-is if analysis failed
        
        # Add enhanced analysis to chunk metadata
        for chunk in chunks:
            if 'enhanced_analysis' not in chunk.metadata:
                chunk.metadata['enhanced_analysis'] = {}
            
            # Add relevant analysis fields
            for key, value in enhanced_analysis.items():
                if key not in ['chunks_analyzed', 'aggregated']:
                    chunk.metadata['enhanced_analysis'][key] = value
            
            # Adjust confidence score
            if enhanced_analysis.get('confidence'):
                chunk.confidence_score = min(chunk.confidence_score, enhanced_analysis['confidence'])
        
        return chunks

    async def _store_chunks(self, chunks: List[CodeChunk], file_path: Path):
        """Store chunks in coordinator's database"""
        if not chunks:
            return
        
        try:
            file_hash = self._generate_file_hash(file_path)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for chunk in chunks:
                cursor.execute("""
                    INSERT OR REPLACE INTO program_chunks 
                    (program_name, chunk_id, chunk_type, content, metadata, 
                     file_hash, confidence_score, line_start, line_end)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    chunk.program_name, chunk.chunk_id, chunk.chunk_type,
                    chunk.content, json.dumps(chunk.metadata), file_hash,
                    chunk.confidence_score, chunk.line_start, chunk.line_end
                ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ Stored {len(chunks)} chunks in coordinator database")
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to store chunks: {str(e)}")

    def _generate_file_hash(self, file_path: Path) -> str:
        """Generate hash for file"""
        try:
            stat_info = file_path.stat()
            hash_input = f"{file_path.name}:{stat_info.st_mtime}:{stat_info.st_size}"
            return hashlib.sha256(hash_input.encode()).hexdigest()
        except Exception:
            return hashlib.sha256(str(file_path).encode()).hexdigest()

    def cleanup(self):
        """ENHANCED: Cleanup method with coordinator and lineage integration"""
        self.logger.info("üßπ Cleaning up FIXED CodeParser Agent...")
        try:
            # Update final statistics
            if hasattr(self, 'coordinator'):
                self.logger.info(f"üìä Final stats: {self._files_processed} files, "
                               f"{self._total_chunks} chunks, {self._api_calls} API calls")
            
            # Call parent cleanup
            super().cleanup()
            
        except Exception as e:
            self.logger.error(f"‚ùå Cleanup error: {e}")

    def get_version_info(self) -> Dict[str, str]:
        """Get version information"""
        return {
            "agent_name": "CodeParserAgent",
            "version": "3.1.0-FIXED-Database-Schema",
            "base_agent": "BaseOpulenceAgent", 
            "deployment_mode": "COORDINATOR_API_LINEAGE_INTEGRATED_FIXED",
            "coordinator_compatible": True,
            "lineage_analyzer_compatible": True,
            "api_based": True,
            "context_window": f"{self.max_context_tokens} tokens",
            "chunking_strategy": "intelligent_content_aware",
            "parsing_approach": "reliable_patterns_first",
            "relationship_extraction": "pattern_based_reliable",
            "supported_file_types": [".cbl", ".cob", ".jcl", ".cpy", ".copy", ".bms", ".sql", ".db2"],
            "database_fixes": [
                "Fixed SQL syntax errors in copybook_relationships table",
                "Added missing replacing_clause and usage_context columns",
                "Fixed duplicate impact_analysis table creation",
                "Corrected all column references in INSERT statements",
                "Added proper error handling for missing columns"
            ],
            "enhanced_features": [
                "Full LineageAnalyzer table support",
                "Field cross-reference tracking",
                "Field usage pattern extraction",
                "Lineage graph node/edge creation",
                "Component lifecycle tracking",
                "Data flow analysis storage",
                "Impact analysis data population",
                "Enhanced file metadata storage"
            ],
            "database_tables": [
                "program_chunks", "program_relationships", "file_access_relationships",
                "copybook_relationships", "field_definitions", "sql_analysis", "llm_analysis_cache",
                "file_metadata", "lineage_nodes", "lineage_edges", "field_usage_tracking",
                "field_cross_reference", "component_lifecycle", "data_flow_analysis",
                "partial_analysis_cache", "impact_analysis"
            ],
            "coordinator_integration": {
                "uses_coordinator_api": True,
                "uses_coordinator_database": True,
                "conservative_token_limits": True,
                "proper_error_handling": True,
                "statistics_reporting": True
            },
            "lineage_integration": {
                "all_required_tables_present": True,
                "field_tracking_enabled": True,
                "graph_node_creation": True,
                "lifecycle_tracking": True,
                "impact_analysis_support": True,
                "cross_program_lineage_ready": True,
                "database_schema_fixed": True
            }
        }

# CRITICAL FIX: Export the correct class name
__all__ = ['CodeParserAgent']