# Opulence Mainframe Deep Research Agent Architecture

## 1. Simple System Overview (Plain English)

The Opulence system takes legacy mainframe code from a **private wealth bank's security transaction processing system** and makes it understandable using modern AI technology:

- **Input**: COBOL programs, JCL job scripts, PROC procedures, DB2 database definitions, and sample transaction data files from the bank's security trading platform
- **Processing**: Parses and loads them into structured format using a code parser and data loader
- **AI Analysis**: Uses a GPU-hosted CodeLLaMA model (exposed via HTTP API) to analyze and summarize complex business logic
- **Orchestration**: A Coordinator Agent manages the workflow across various specialized research agents
- **Output**: Provides lineage maps showing how customer data flows, business logic summaries explaining trading rules, comprehensive documentation, and an interactive chat interface for asking questions

**Example Scenario**: Understanding how a customer's security purchase order flows through 50+ COBOL programs, what validation rules apply, and how it updates the portfolio database.

---

## 2. Core Components (80/20 Rule Table)

| Component               | Function                                     | 80/20 Value                                                 |
|-------------------------|----------------------------------------------|-------------------------------------------------------------|
| **Code Parser**         | Converts COBOL/JCL into structured AST        | Enables structured understanding of 40-year-old trading logic |
| **Data Loader**         | Loads DB2 tables and sample transaction files | Adds real-world context from actual customer trades         |
| **Vector Index Agent**  | Embeds and indexes all elements in FAISS     | Powers fast semantic search: "find all margin calculation logic" |
| **Lineage Agent**       | Tracks fields across jobs and programs        | Critical for compliance: trace customer ID through entire system |
| **Logic Analyzer Agent**| Extracts business logic and conditional rules | Automates discovery of trading rules and validation logic   |
| **Documentation Agent** | Summarizes components and logic               | Generates readable docs explaining arcane settlement processes |
| **Chat Agent**          | Interfaces with user to answer questions      | "How does stop-loss order processing work?" gets instant answers |
| **Coordinator Agent**   | Orchestrates flow and agent sequencing        | Ensures systematic analysis of interconnected trading systems |
| **GPU LLM API**         | CodeLLaMA exposed via API for summarization  | Core intelligence for understanding legacy financial code    |

---

## 3. System Flow and Individual Agent Workflows

### Overall System Architecture Flow

```mermaid
graph TB
    subgraph "Input Layer"
        A[COBOL Programs] 
        B[JCL Jobs]
        C[DB2 DDL]
        D[CSV Data]
        E[DCLGEN Files]
    end
    
    subgraph "Processing Layer"
        F[Code Parser Agent]
        G[Data Loader Agent] 
        H[Vector Index Agent]
        I[Lineage Analyzer Agent]
        J[Logic Analyzer Agent]
        K[Documentation Agent]
        L[Chat Agent]
    end
    
    subgraph "Coordinator Layer"
        M[API Coordinator]
        N[Load Balancer]
        O[GPU API Servers]
    end
    
    subgraph "Storage Layer"
        P[SQLite Database]
        Q[FAISS Index]
        R[ChromaDB]
    end
    
    subgraph "Output Layer"
        S[Lineage Reports]
        T[Documentation]
        U[Chat Responses]
        V[Analysis Reports]
    end
    
    A --> F
    B --> F
    C --> G
    D --> G
    E --> G
    
    F --> P
    G --> P
    F --> H
    G --> H
    
    H --> Q
    H --> R
    
    P --> I
    P --> J
    Q --> I
    Q --> J
    
    I --> K
    J --> K
    
    K --> T
    I --> S
    J --> V
    L --> U
    
    M --> N
    N --> O
    O --> F
    O --> G
    O --> I
    O --> J
    O --> K
    O --> L
```

---

## 4. Individual Agent Workflows

### 4.1 Code Parser Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        A1[SECTRD01.cbl<br/>Security Trading Program]
        A2[PORTFOLIO.jcl<br/>Portfolio Update Job]
        A3[CUSTMAST.proc<br/>Customer Master Procedure]
        A4[SETTLEMENT.cbl<br/>Settlement Processing]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        B1[­ЪЊё File Type Detection<br/>Рђб COBOL vs JCL vs PROC<br/>Рђб Business rule validation<br/>Рђб Structure verification] --> B2[­ЪћЇ Content Parsing<br/>Рђб Extract divisions/sections<br/>Рђб Parse data definitions<br/>Рђб Identify paragraphs<br/>Рђб Extract PERFORM calls] --> B3[­ЪДа API-Based Analysis<br/>Рђб Send code to CodeLLaMA<br/>Рђб Extract business logic<br/>Рђб Identify patterns<br/>Рђб Generate descriptions] --> B4[­ЪЊі Chunk Creation<br/>Рђб Create structured chunks<br/>Рђб Add business context<br/>Рђб Generate metadata<br/>Рђб Calculate complexity] --> B5[­ЪњЙ Database Storage<br/>Рђб Store in SQLite<br/>Рђб Create relationships<br/>Рђб Index for search<br/>Рђб Validate integrity]
    end
    
    subgraph "Outputs (Bottom Right)"
        C1[­ЪЊІ Structured Chunks<br/>Рђб 2,500 code segments<br/>Рђб Business context metadata<br/>Рђб Complexity scores]
        
        C2[­ЪЌё№ИЈ Database Records<br/>Рђб program_chunks table<br/>Рђб Field lineage data<br/>Рђб Control flow paths]
        
        C3[­ЪЊѕ Analysis Metrics<br/>Рђб Complexity: 6.2/10<br/>Рђб Business rules: 15<br/>Рђб Performance issues: 3]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B1
    A4 --> B1
    
    B5 --> C1
    B5 --> C2
    B5 --> C3
    
    style A1 fill:#e1f5fe
    style A2 fill:#e1f5fe
    style A3 fill:#e1f5fe
    style A4 fill:#e1f5fe
    style C1 fill:#e8f5e8
    style C2 fill:#e8f5e8
    style C3 fill:#e8f5e8
```

### 4.2 Data Loader Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        D1[SECURITY_TXN.ddl<br/>Transaction Table Definition]
        D2[customer_data.csv<br/>Customer Master Data]
        D3[CUSTOMER_RECORD.cpy<br/>COBOL Copybook]
        D4[trades_sample.csv<br/>10000 Transaction Records]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        E1[­ЪћЇ File Analysis<br/>Рђб Detect CSV vs DDL vs Copybook<br/>Рђб Analyze structure patterns<br/>Рђб Validate data formats<br/>Рђб Estimate complexity] --> E2[­ЪЊІ Schema Generation<br/>Рђб Infer column types<br/>Рђб Map COBOL PIC to SQL<br/>Рђб Extract field relationships<br/>Рђб Create constraints] --> E3[­ЪДа API Enhancement<br/>Рђб Generate field descriptions<br/>Рђб Classify data types<br/>Рђб Identify business meaning<br/>Рђб Add quality metrics] --> E4[­ЪЈЌ№ИЈ Table Creation<br/>Рђб Create SQLite tables<br/>Рђб Load sample data<br/>Рђб Establish indexes<br/>Рђб Validate integrity] --> E5[­ЪЊі Quality Analysis<br/>Рђб Calculate completeness<br/>Рђб Check data consistency<br/>Рђб Identify anomalies<br/>Рђб Generate metrics]
    end
    
    subgraph "Outputs (Bottom Right)"
        F1[­ЪЌЃ№ИЈ Data Tables<br/>Рђб SECURITY_TXN 50 fields<br/>Рђб CUSTOMER_DATA 25 fields<br/>Рђб Sample data loaded]
        
        F2[­ЪЊќ Data Catalog<br/>Рђб Field descriptions<br/>Рђб Business classifications<br/>Рђб Quality scores 0.85/1.0]
        
        F3[­ЪћЌ Lineage Metadata<br/>Рђб Source file mappings<br/>Рђб Field relationships<br/>Рђб Dependencies tracked]
    end
    
    D1 --> E1
    D2 --> E1
    D3 --> E1
    D4 --> E1
    
    E5 --> F1
    E5 --> F2
    E5 --> F3
    
    style D1 fill:#fff3e0
    style D2 fill:#fff3e0
    style D3 fill:#fff3e0
    style D4 fill:#fff3e0
    style F1 fill:#f3e5f5
    style F2 fill:#f3e5f5
    style F3 fill:#f3e5f5
```

### 4.3 Vector Index Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        G1[Parsed Code Chunks<br/>Рђб 2,500 COBOL segments<br/>Рђб Business context metadata<br/>Рђб Field definitions]
        G2[Local CodeBERT Model<br/>Рђб microsoft/codebert-base<br/>Рђб CPU-based processing<br/>Рђб Airgap compatible]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        H1[­ЪћД Model Initialization<br/>Рђб Load CodeBERT on CPU<br/>Рђб Initialize tokenizer<br/>Рђб Setup embedding function<br/>Рђб Avoid GPU conflicts] --> H2[РџА Embedding Generation<br/>Рђб Process chunks in batches<br/>Рђб Generate 768-dim vectors<br/>Рђб Normalize for similarity<br/>Рђб Add business context] --> H3[­ЪЌѓ№ИЈ Index Creation<br/>Рђб Build FAISS index<br/>Рђб Store in ChromaDB<br/>Рђб Create relationships<br/>Рђб Optimize for search] --> H4[­ЪћЇ Search Capabilities<br/>Рђб Semantic similarity<br/>Рђб Business logic patterns<br/>Рђб Code functionality<br/>Рђб Cross-component analysis] --> H5[­ЪњЙ Persistence<br/>Рђб Save FAISS index<br/>Рђб Store embeddings<br/>Рђб Maintain metadata<br/>Рђб Enable incremental updates]
    end
    
    subgraph "Outputs (Bottom Right)"
        I1[­Ъј» FAISS Index<br/>Рђб 2,500 vectors stored<br/>Рђб Sub-second search<br/>Рђб Cosine similarity]
        
        I2[­ЪћЇ Search Results<br/>Рђб Semantic code search<br/>Рђб Similarity scores<br/>Рђб Related components]
        
        I3[­Ъїљ Knowledge Graph<br/>Рђб Component relationships<br/>Рђб Code pattern clusters<br/>Рђб Dependency mappings]
    end
    
    G1 --> H1
    G2 --> H1
    
    H5 --> I1
    H5 --> I2
    H5 --> I3
    
    style G1 fill:#e8eaf6
    style G2 fill:#e8eaf6
    style I1 fill:#fff8e1
    style I2 fill:#fff8e1
    style I3 fill:#fff8e1
```

### 4.4 Lineage Analyzer Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        J1[Field References<br/>Рђб CUSTOMER-ID usage<br/>Рђб TRADE-AMOUNT flows<br/>Рђб ACCOUNT-BALANCE updates]
        J2[Program Relationships<br/>Рђб CALL statements<br/>Рђб PERFORM references<br/>Рђб File operations]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        K1[­ЪћЇ Reference Discovery<br/>Рђб Search code patterns<br/>Рђб Extract field usage<br/>Рђб Map data flows<br/>Рђб Identify transformations] --> K2[­ЪДа API Analysis<br/>Рђб Analyze usage context<br/>Рђб Extract business logic<br/>Рђб Determine data flow<br/>Рђб Classify operations] --> K3[­ЪЊі Impact Assessment<br/>Рђб Calculate complexity<br/>Рђб Assess risk levels<br/>Рђб Identify dependencies<br/>Рђб Generate recommendations] --> K4[­ЪЌ║№ИЈ Lineage Mapping<br/>Рђб Create flow diagrams<br/>Рђб Build dependency graph<br/>Рђб Track lifecycle stages<br/>Рђб Document relationships] --> K5[­ЪЊІ Report Generation<br/>Рђб Compile findings<br/>Рђб Generate summaries<br/>Рђб Create recommendations<br/>Рђб Export lineage data]
    end
    
    subgraph "Outputs (Bottom Right)"
        L1[­ЪЌ║№ИЈ Lineage Maps<br/>Рђб CUSTOMER-ID: 15 programs<br/>Рђб 45 total references<br/>Рђб Complete data flow]
        
        L2[Рџа№ИЈ Impact Analysis<br/>Рђб Risk Level: MEDIUM<br/>Рђб 8 affected programs<br/>Рђб Change recommendations]
        
        L3[­ЪЊі Lifecycle Reports<br/>Рђб Creation Рєњ Usage Рєњ Archive<br/>Рђб Business context<br/>Рђб Compliance tracking]
    end
    
    J1 --> K1
    J2 --> K1
    
    K5 --> L1
    K5 --> L2
    K5 --> L3
    
    style J1 fill:#e0f2f1
    style J2 fill:#e0f2f1
    style L1 fill:#fce4ec
    style L2 fill:#fce4ec
    style L3 fill:#fce4ec
```

### 4.5 Logic Analyzer Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        M1[COBOL Programs<br/>Рђб Business logic chunks<br/>Рђб Conditional statements<br/>Рђб Calculation rules]
        M2[JCL Job Flows<br/>Рђб Step dependencies<br/>Рђб Control statements<br/>Рђб Error handling]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        N1[­ЪћЇ Pattern Detection<br/>Рђб Identify IF-THEN logic<br/>Рђб Extract calculations<br/>Рђб Find validation rules<br/>Рђб Map control flow] --> N2[­ЪД« Complexity Analysis<br/>Рђб Calculate cyclomatic complexity<br/>Рђб Assess nesting levels<br/>Рђб Count decision points<br/>Рђб Evaluate maintainability] --> N3[­ЪДа API Logic Extraction<br/>Рђб Extract business rules<br/>Рђб Identify optimization opportunities<br/>Рђб Generate explanations<br/>Рђб Document processes] --> N4[­ЪЊі Quality Assessment<br/>Рђб Code quality metrics<br/>Рђб Performance analysis<br/>Рђб Risk identification<br/>Рђб Best practice evaluation] --> N5[­ЪЊІ Recommendation Engine<br/>Рђб Generate improvements<br/>Рђб Suggest refactoring<br/>Рђб Identify technical debt<br/>Рђб Prioritize changes]
    end
    
    subgraph "Outputs (Bottom Right)"
        O1[­ЪЊі Logic Analysis<br/>Рђб 15 business rules found<br/>Рђб Complexity score: 6.2/10<br/>Рђб 3 optimization opportunities]
        
        O2[­ЪћД Recommendations<br/>Рђб Refactor 3 high-complexity methods<br/>Рђб Add error handling<br/>Рђб Optimize loops]
        
        O3[­ЪЊѕ Quality Metrics<br/>Рђб Maintainability: 7.5/10<br/>Рђб Code quality: 8.1/10<br/>Рђб Technical debt: Medium]
    end
    
    M1 --> N1
    M2 --> N1
    
    N5 --> O1
    N5 --> O2
    N5 --> O3
    
    style M1 fill:#f1f8e9
    style M2 fill:#f1f8e9
    style O1 fill:#e3f2fd
    style O2 fill:#e3f2fd
    style O3 fill:#e3f2fd
```

### 4.6 Documentation Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        P1[Analysis Results<br/>Рђб Logic analysis data<br/>Рђб Lineage mappings<br/>Рђб Code complexity metrics]
        P2[Business Context<br/>Рђб Field classifications<br/>Рђб Process descriptions<br/>Рђб Compliance requirements]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        Q1[­ЪЊІ Content Aggregation<br/>Рђб Collect analysis results<br/>Рђб Merge related data<br/>Рђб Organize by component<br/>Рђб Structure information] --> Q2[­ЪДа API Documentation<br/>Рђб Generate descriptions<br/>Рђб Create summaries<br/>Рђб Explain processes<br/>Рђб Add context] --> Q3[­ЪЊЮ Format Generation<br/>Рђб Create markdown docs<br/>Рђб Generate HTML reports<br/>Рђб Build cross-references<br/>Рђб Add navigation] --> Q4[­Ъје Enhancement<br/>Рђб Add diagrams<br/>Рђб Include examples<br/>Рђб Create glossaries<br/>Рђб Improve readability] --> Q5[РюЁ Validation<br/>Рђб Check completeness<br/>Рђб Verify accuracy<br/>Рђб Ensure consistency<br/>Рђб Final review]
    end
    
    subgraph "Outputs (Bottom Right)"
        R1[­ЪЊџ Technical Documentation<br/>Рђб 50+ pages generated<br/>Рђб Component descriptions<br/>Рђб Process flows]
        
        R2[­ЪЊі Executive Reports<br/>Рђб System overview<br/>Рђб Risk assessments<br/>Рђб Recommendations]
        
        R3[­ЪћЌ Interactive Docs<br/>Рђб Searchable content<br/>Рђб Cross-referenced<br/>Рђб Hyperlinked navigation]
    end
    
    P1 --> Q1
    P2 --> Q1
    
    Q5 --> R1
    Q5 --> R2
    Q5 --> R3
    
    style P1 fill:#f9fbe7
    style P2 fill:#f9fbe7
    style R1 fill:#ede7f6
    style R2 fill:#ede7f6
    style R3 fill:#ede7f6
```

### 4.7 Chat Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        S1[User Question<br/>How does the system<br/>handle large sell orders?]
        S2[Context Data<br/>Рђб Conversation history<br/>Рђб Available analysis<br/>Рђб System knowledge]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        T1[­ЪДЕ Query Classification<br/>Рђб Identify intent type<br/>Рђб Extract components<br/>Рђб Determine complexity<br/>Рђб Plan response strategy] --> T2[­ЪћЇ Context Gathering<br/>Рђб Search vector index<br/>Рђб Query databases<br/>Рђб Get analysis results<br/>Рђб Collect relevant data] --> T3[­ЪДа API Response Generation<br/>Рђб Generate explanation<br/>Рђб Add technical details<br/>Рђб Include examples<br/>Рђб Provide guidance] --> T4[­ЪњА Enhancement<br/>Рђб Add suggestions<br/>Рђб Include references<br/>Рђб Format response<br/>Рђб Ensure clarity] --> T5[РюЁ Quality Check<br/>Рђб Validate accuracy<br/>Рђб Check completeness<br/>Рђб Ensure helpfulness<br/>Рђб Final formatting]
    end
    
    subgraph "Outputs (Bottom Right)"
        U1[­Ъњг Intelligent Response<br/>Large sell orders trigger<br/>LARGE-ORDER-CHECK<br/>validation in SECTRD01]
        
        U2[­ЪЊІ Follow-up Suggestions<br/>Рђб Show validation logic<br/>Рђб Analyze risk controls<br/>Рђб Find similar patterns]
        
        U3[­ЪћЌ Context Links<br/>Рђб Related components<br/>Рђб Additional resources<br/>Рђб Cross-references]
    end
    
    S1 --> T1
    S2 --> T1
    
    T5 --> U1
    T5 --> U2
    T5 --> U3
    
    style S1 fill:#e1f5fe
    style S2 fill:#e1f5fe
    style U1 fill:#e8f5e8
    style U2 fill:#e8f5e8
    style U3 fill:#e8f5e8
```

---

## 5. Agent Coordination Flow

```mermaid
sequenceDiagram
    participant User
    participant Coordinator
    participant CodeParser
    participant DataLoader
    participant VectorIndex
    participant Lineage
    participant Logic
    participant Docs
    participant Chat
    participant GPU_API
    
    User->>Coordinator: Upload security_trading.cbl
    Coordinator->>CodeParser: Process COBOL file
    CodeParser->>GPU_API: Analyze business logic
    GPU_API-->>CodeParser: Business rule extraction
    CodeParser-->>Coordinator: 250 code chunks created
    
    Coordinator->>DataLoader: Process transaction.csv
    DataLoader->>GPU_API: Generate field descriptions
    GPU_API-->>DataLoader: Enhanced schema
    DataLoader-->>Coordinator: Tables created, data loaded
    
    Coordinator->>VectorIndex: Index all chunks
    VectorIndex->>VectorIndex: Generate embeddings (local)
    VectorIndex-->>Coordinator: FAISS index ready
    
    User->>Chat: "Analyze CUSTOMER-ID lineage"
    Chat->>Coordinator: Request lineage analysis
    Coordinator->>Lineage: Analyze CUSTOMER-ID
    Lineage->>GPU_API: Analyze field usage patterns
    GPU_API-->>Lineage: Usage context analysis
    Lineage-->>Chat: Lineage map with 15 programs
    Chat->>GPU_API: Generate response
    GPU_API-->>Chat: Natural language explanation
    Chat-->>User: "CUSTOMER-ID flows through..."
```

---

## 6. Output Artifacts

The Opulence system produces these deliverables for the bank's security trading system:

РюЁ **Field-level data lineage reports**  
   - "CUSTOMER-ID flows from CUSTMAST Рєњ SECTRD01 Рєњ PORTFOLIO-UPDATE Рєњ TRADE-HISTORY"
   - Compliance-ready audit trails

РюЁ **Extracted business logic summaries**  
   - "Stop-loss orders: IF CURRENT-PRICE < (STOP-PRICE * 0.95) THEN EXECUTE-SELL"
   - Trading rule documentation in plain English

РюЁ **Annotated markdown documentation of code modules**  
   - Complete explanation of settlement processing
   - Cross-references between related programs

РюЁ **Interactive chat interface for querying understanding**  
   - "What happens when a trade fails settlement?"
   - "Show me all programs that update customer portfolios"

---

## 7. Sample Data Context: Private Wealth Bank Security Transactions

### Input Files for Analysis:

**COBOL Programs:**
- `SECTRD01.cbl` - Main security trading program (2,500 lines)
- `VALIDATE.cbl` - Order validation logic (800 lines)  
- `SETTLE.cbl` - Settlement processing (1,200 lines)
- `PORTFOLIO.cbl` - Portfolio update logic (900 lines)

**JCL Jobs:**
- `DAILYTRD.jcl` - Daily trade processing batch job
- `SETTLEMENT.jcl` - End-of-day settlement job
- `RECON.jcl` - Trade reconciliation job

**DB2 Tables:**
```sql
-- SECURITY_TRANSACTION table
CREATE TABLE SECURITY_TXN (
    CUST_ID        CHAR(10),
    TRADE_ID       CHAR(15),
    SECURITY_CODE  CHAR(8),
    TRADE_TYPE     CHAR(4),    -- BUY/SELL
    QUANTITY       DECIMAL(15,2),
    PRICE          DECIMAL(15,4),
    TRADE_DATE     DATE,
    SETTLE_DATE    DATE,
    STATUS         CHAR(3)     -- PEN/SET/FAI
);
```

**Sample Transaction Data:**
```csv
CUST_ID,TRADE_ID,SECURITY_CODE,TRADE_TYPE,QUANTITY,PRICE,TRADE_DATE,STATUS
PWB0001234,TRD20241201001,AAPL,BUY,100,150.25,2024-12-01,PEN
PWB0001234,TRD20241201002,TSLA,SELL,50,245.80,2024-12-01,SET
PWB0001567,TRD20241201003,MSFT,BUY,200,380.15,2024-12-01,FAI
```

---

## 8. Individual Agent Explanations

### Vector Index Agent
**Purpose**: Creates searchable embeddings of all code segments and business logic.

**Bank Example**: When analyzing the security trading system, this agent:
- Embeds all COBOL paragraphs dealing with order validation
- Creates vectors for trading rule conditions  
- Enables semantic search like "find all margin calculation logic"

**API Integration**: Makes HTTP calls to CodeLLaMA to generate embeddings and understand code semantics.

### Lineage Agent  
**Purpose**: Tracks how data fields flow through the entire system.

**Bank Example**: For a customer security purchase:
1. **CUSTOMER-ID** enters via online trading platform
2. Flows through `VALIDATE.cbl` for KYC checks
3. Processed in `SECTRD01.cbl` for order execution
4. Updates `PORTFOLIO.cbl` for position management
5. Records in `TRADE-HISTORY` table for audit

**Critical for Compliance**: Regulators require complete audit trails showing how customer data is processed.

### Logic Analyzer Agent
**Purpose**: Extracts and explains complex business rules embedded in COBOL logic.

**Bank Example**: Discovers trading rules like:
```cobol
IF TRADE-AMOUNT > DAILY-LIMIT
   AND CUSTOMER-TIER NOT = 'PLATINUM'
   THEN MOVE 'HOLD' TO TRADE-STATUS
   PERFORM MANUAL-APPROVAL-PROCESS
```

Translates to: "Trades over daily limit require manual approval unless customer is Platinum tier."

### Documentation Agent
**Purpose**: Creates human-readable documentation explaining system functionality.

**Bank Example**: Generates documentation like:
- "Settlement Process Overview: How T+2 settlement works"
- "Stop-Loss Order Processing: Automated selling when price thresholds are breached"
- "Customer Portfolio Updates: Real-time vs. batch processing logic"

### Chat Agent
**Purpose**: Provides conversational interface for querying system knowledge.

**Bank Example Queries**:
- "How does the system handle partial fills on large orders?"
- "What validation checks are performed before executing a trade?"
- "Show me the settlement process for international securities"

**Response Example**: "When a large order cannot be filled completely, the PARTIAL-FILL-HANDLER in SECTRD01 splits it into smaller chunks and processes them separately, updating the customer's available cash after each partial execution..."

---

## 9. Coordination Flow: Processing a Security Transaction

### Real-World Scenario: Customer Places $500K Apple Stock Purchase

1. **File Processing Phase**:
   - Code Parser analyzes `SECTRD01.cbl` and extracts order processing logic
   - Data Loader imports recent Apple trading data and customer portfolio info
   - System identifies all programs involved in large order processing

2. **Analysis Phase**:
   - **Vector Index Agent**: Finds all code segments related to large order handling
   - **Lineage Agent**: Maps how customer cash balance flows through the system
   - **Logic Analyzer**: Extracts validation rules for large orders (credit checks, position limits)
   - **Documentation Agent**: Summarizes the complete order-to-settlement workflow

3. **Query Phase**:
   - Risk manager asks: "What approvals are needed for this trade size?"
   - Chat Agent searches indexed knowledge and responds: "Orders over $250K require senior trader approval per LARGE-ORDER-CHECK paragraph, plus real-time margin calculation..."

4. **Compliance Phase**:
   - Lineage reports show complete audit trail
   - Logic summaries document all decision points
   - Documentation provides regulatory-compliant process descriptions

This architecture transforms decades-old, undocumented mainframe code into an accessible, searchable knowledge base that supports both operational teams and regulatory compliance requirements.

---

## 10. Technical Implementation Notes

### API-Based Architecture
The Opulence system uses HTTP APIs to communicate with GPU-hosted CodeLLaMA models, enabling:
- **Scalability**: Multiple model servers can handle concurrent analysis requests
- **Load Balancing**: Requests are distributed across available GPU resources
- **Fault Tolerance**: Circuit breakers and retry logic ensure robust operation
- **Resource Efficiency**: No need for local GPU allocation per agent

### Database Design
SQLite database stores:
- **program_chunks**: Parsed code segments with metadata
- **field_lineage**: Data flow tracking for compliance
- **vector_embeddings**: FAISS index references for semantic search
- **processing_stats**: Performance monitoring and audit trails

This architecture enables financial institutions to understand and maintain critical legacy systems while meeting modern regulatory and operational requirements.