# Opulence Mainframe Deep Research Agent Architecture

## 1. Simple System Overview (Plain English)

The Opulence system takes legacy mainframe code from a **private wealth bank's security transaction processing system** and makes it understandable using modern AI technology:

- **Input**: COBOL programs, JCL job scripts, PROC procedures, DB2 database definitions, and sample transaction data files from the bank's security trading platform
- **Processing**: Parses and loads them into structured format using a code parser and data loader
- **AI Analysis**: Uses a GPU-hosted CodeLLaMA model (exposed via HTTP API) to analyze and summarize complex business logic
- **Orchestration**: A Coordinator Agent manages the workflow across various specialized research agents
- **Output**: Provides lineage maps showing how customer data flows, business logic summaries explaining trading rules, comprehensive documentation, and an interactive chat interface for asking questions

**Example Scenario**: Understanding how a customer's security purchase order flows through 50+ COBOL programs, what validation rules apply, and how it updates the portfolio database.

---

## 2. Core Components (80/20 Rule Table)

| Component               | Function                                     | 80/20 Value                                                 |
|-------------------------|----------------------------------------------|-------------------------------------------------------------|
| **Code Parser**         | Converts COBOL/JCL into structured AST        | Enables structured understanding of 40-year-old trading logic |
| **Data Loader**         | Loads DB2 tables and sample transaction files | Adds real-world context from actual customer trades         |
| **Vector Index Agent**  | Embeds and indexes all elements in FAISS     | Powers fast semantic search: "find all margin calculation logic" |
| **Lineage Agent**       | Tracks fields across jobs and programs        | Critical for compliance: trace customer ID through entire system |
| **Logic Analyzer Agent**| Extracts business logic and conditional rules | Automates discovery of trading rules and validation logic   |
| **Documentation Agent** | Summarizes components and logic               | Generates readable docs explaining arcane settlement processes |
| **Chat Agent**          | Interfaces with user to answer questions      | "How does stop-loss order processing work?" gets instant answers |
| **Coordinator Agent**   | Orchestrates flow and agent sequencing        | Ensures systematic analysis of interconnected trading systems |
| **GPU LLM API**         | CodeLLaMA exposed via API for summarization  | Core intelligence for understanding legacy financial code    |

---

## 3. System Flow and Individual Agent Workflows

### Overall System Architecture Flow

```mermaid
graph TB
    subgraph "Input Layer"
        A[COBOL Programs] 
        B[JCL Jobs]
        C[DB2 DDL]
        D[CSV Data]
        E[DCLGEN Files]
    end
    
    subgraph "Processing Layer"
        F[Code Parser Agent]
        G[Data Loader Agent] 
        H[Vector Index Agent]
        I[Lineage Analyzer Agent]
        J[Logic Analyzer Agent]
        K[Documentation Agent]
        L[Chat Agent]
    end
    
    subgraph "Coordinator Layer"
        M[API Coordinator]
        N[Load Balancer]
        O[GPU API Servers]
    end
    
    subgraph "Storage Layer"
        P[SQLite Database]
        Q[FAISS Index]
        R[ChromaDB]
    end
    
    subgraph "Output Layer"
        S[Lineage Reports]
        T[Documentation]
        U[Chat Responses]
        V[Analysis Reports]
    end
    
    A --> F
    B --> F
    C --> G
    D --> G
    E --> G
    
    F --> P
    G --> P
    F --> H
    G --> H
    
    H --> Q
    H --> R
    
    P --> I
    P --> J
    Q --> I
    Q --> J
    
    I --> K
    J --> K
    
    K --> T
    I --> S
    J --> V
    L --> U
    
    M --> N
    N --> O
    O --> F
    O --> G
    O --> I
    O --> J
    O --> K
    O --> L
```

---

## 4. Individual Agent Workflows

### 4.1 Code Parser Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        A1[SECTRD01.cbl<br/>Security Trading Program]
        A2[PORTFOLIO.jcl<br/>Portfolio Update Job]
        A3[CUSTMAST.proc<br/>Customer Master Procedure]
        A4[SETTLEMENT.cbl<br/>Settlement Processing]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        B1[ğŸ“„ File Type Detection<br/>â€¢ COBOL vs JCL vs PROC<br/>â€¢ Business rule validation<br/>â€¢ Structure verification] --> B2[ğŸ” Content Parsing<br/>â€¢ Extract divisions/sections<br/>â€¢ Parse data definitions<br/>â€¢ Identify paragraphs<br/>â€¢ Extract PERFORM calls] --> B3[ğŸ§  API-Based Analysis<br/>â€¢ Send code to CodeLLaMA<br/>â€¢ Extract business logic<br/>â€¢ Identify patterns<br/>â€¢ Generate descriptions] --> B4[ğŸ“Š Chunk Creation<br/>â€¢ Create structured chunks<br/>â€¢ Add business context<br/>â€¢ Generate metadata<br/>â€¢ Calculate complexity] --> B5[ğŸ’¾ Database Storage<br/>â€¢ Store in SQLite<br/>â€¢ Create relationships<br/>â€¢ Index for search<br/>â€¢ Validate integrity]
    end
    
    subgraph "Outputs (Bottom Right)"
        C1[ğŸ“‹ Structured Chunks<br/>â€¢ 2,500 code segments<br/>â€¢ Business context metadata<br/>â€¢ Complexity scores]
        
        C2[ğŸ—„ï¸ Database Records<br/>â€¢ program_chunks table<br/>â€¢ Field lineage data<br/>â€¢ Control flow paths]
        
        C3[ğŸ“ˆ Analysis Metrics<br/>â€¢ Complexity: 6.2/10<br/>â€¢ Business rules: 15<br/>â€¢ Performance issues: 3]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B1
    A4 --> B1
    
    B5 --> C1
    B5 --> C2
    B5 --> C3
    
    style A1 fill:#e1f5fe
    style A2 fill:#e1f5fe
    style A3 fill:#e1f5fe
    style A4 fill:#e1f5fe
    style C1 fill:#e8f5e8
    style C2 fill:#e8f5e8
    style C3 fill:#e8f5e8
```

### 4.2 Data Loader Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        D1[SECURITY_TXN.ddl<br/>Transaction Table Definition]
        D2[customer_data.csv<br/>Customer Master Data]
        D3[CUSTOMER_RECORD.cpy<br/>COBOL Copybook]
        D4[trades_sample.csv<br/>10000 Transaction Records]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        E1[ğŸ” File Analysis<br/>â€¢ Detect CSV vs DDL vs Copybook<br/>â€¢ Analyze structure patterns<br/>â€¢ Validate data formats<br/>â€¢ Estimate complexity] --> E2[ğŸ“‹ Schema Generation<br/>â€¢ Infer column types<br/>â€¢ Map COBOL PIC to SQL<br/>â€¢ Extract field relationships<br/>â€¢ Create constraints] --> E3[ğŸ§  API Enhancement<br/>â€¢ Generate field descriptions<br/>â€¢ Classify data types<br/>â€¢ Identify business meaning<br/>â€¢ Add quality metrics] --> E4[ğŸ—ï¸ Table Creation<br/>â€¢ Create SQLite tables<br/>â€¢ Load sample data<br/>â€¢ Establish indexes<br/>â€¢ Validate integrity] --> E5[ğŸ“Š Quality Analysis<br/>â€¢ Calculate completeness<br/>â€¢ Check data consistency<br/>â€¢ Identify anomalies<br/>â€¢ Generate metrics]
    end
    
    subgraph "Outputs (Bottom Right)"
        F1[ğŸ—ƒï¸ Data Tables<br/>â€¢ SECURITY_TXN 50 fields<br/>â€¢ CUSTOMER_DATA 25 fields<br/>â€¢ Sample data loaded]
        
        F2[ğŸ“– Data Catalog<br/>â€¢ Field descriptions<br/>â€¢ Business classifications<br/>â€¢ Quality scores 0.85/1.0]
        
        F3[ğŸ”— Lineage Metadata<br/>â€¢ Source file mappings<br/>â€¢ Field relationships<br/>â€¢ Dependencies tracked]
    end
    
    D1 --> E1
    D2 --> E1
    D3 --> E1
    D4 --> E1
    
    E5 --> F1
    E5 --> F2
    E5 --> F3
    
    style D1 fill:#fff3e0
    style D2 fill:#fff3e0
    style D3 fill:#fff3e0
    style D4 fill:#fff3e0
    style F1 fill:#f3e5f5
    style F2 fill:#f3e5f5
    style F3 fill:#f3e5f5
```

### 4.3 Vector Index Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        G1[Parsed Code Chunks<br/>â€¢ 2,500 COBOL segments<br/>â€¢ Business context metadata<br/>â€¢ Field definitions]
        G2[Local CodeBERT Model<br/>â€¢ microsoft/codebert-base<br/>â€¢ CPU-based processing<br/>â€¢ Airgap compatible]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        H1[ğŸ”§ Model Initialization<br/>â€¢ Load CodeBERT on CPU<br/>â€¢ Initialize tokenizer<br/>â€¢ Setup embedding function<br/>â€¢ Avoid GPU conflicts] --> H2[âš¡ Embedding Generation<br/>â€¢ Process chunks in batches<br/>â€¢ Generate 768-dim vectors<br/>â€¢ Normalize for similarity<br/>â€¢ Add business context] --> H3[ğŸ—‚ï¸ Index Creation<br/>â€¢ Build FAISS index<br/>â€¢ Store in ChromaDB<br/>â€¢ Create relationships<br/>â€¢ Optimize for search] --> H4[ğŸ” Search Capabilities<br/>â€¢ Semantic similarity<br/>â€¢ Business logic patterns<br/>â€¢ Code functionality<br/>â€¢ Cross-component analysis] --> H5[ğŸ’¾ Persistence<br/>â€¢ Save FAISS index<br/>â€¢ Store embeddings<br/>â€¢ Maintain metadata<br/>â€¢ Enable incremental updates]
    end
    
    subgraph "Outputs (Bottom Right)"
        I1[ğŸ¯ FAISS Index<br/>â€¢ 2,500 vectors stored<br/>â€¢ Sub-second search<br/>â€¢ Cosine similarity]
        
        I2[ğŸ” Search Results<br/>â€¢ Semantic code search<br/>â€¢ Similarity scores<br/>â€¢ Related components]
        
        I3[ğŸŒ Knowledge Graph<br/>â€¢ Component relationships<br/>â€¢ Code pattern clusters<br/>â€¢ Dependency mappings]
    end
    
    G1 --> H1
    G2 --> H1
    
    H5 --> I1
    H5 --> I2
    H5 --> I3
    
    style G1 fill:#e8eaf6
    style G2 fill:#e8eaf6
    style I1 fill:#fff8e1
    style I2 fill:#fff8e1
    style I3 fill:#fff8e1
```

### 4.4 Lineage Analyzer Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        J1[Field References<br/>â€¢ CUSTOMER-ID usage<br/>â€¢ TRADE-AMOUNT flows<br/>â€¢ ACCOUNT-BALANCE updates]
        J2[Program Relationships<br/>â€¢ CALL statements<br/>â€¢ PERFORM references<br/>â€¢ File operations]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        K1[ğŸ” Reference Discovery<br/>â€¢ Search code patterns<br/>â€¢ Extract field usage<br/>â€¢ Map data flows<br/>â€¢ Identify transformations] --> K2[ğŸ§  API Analysis<br/>â€¢ Analyze usage context<br/>â€¢ Extract business logic<br/>â€¢ Determine data flow<br/>â€¢ Classify operations] --> K3[ğŸ“Š Impact Assessment<br/>â€¢ Calculate complexity<br/>â€¢ Assess risk levels<br/>â€¢ Identify dependencies<br/>â€¢ Generate recommendations] --> K4[ğŸ—ºï¸ Lineage Mapping<br/>â€¢ Create flow diagrams<br/>â€¢ Build dependency graph<br/>â€¢ Track lifecycle stages<br/>â€¢ Document relationships] --> K5[ğŸ“‹ Report Generation<br/>â€¢ Compile findings<br/>â€¢ Generate summaries<br/>â€¢ Create recommendations<br/>â€¢ Export lineage data]
    end
    
    subgraph "Outputs (Bottom Right)"
        L1[ğŸ—ºï¸ Lineage Maps<br/>â€¢ CUSTOMER-ID: 15 programs<br/>â€¢ 45 total references<br/>â€¢ Complete data flow]
        
        L2[âš ï¸ Impact Analysis<br/>â€¢ Risk Level: MEDIUM<br/>â€¢ 8 affected programs<br/>â€¢ Change recommendations]
        
        L3[ğŸ“Š Lifecycle Reports<br/>â€¢ Creation â†’ Usage â†’ Archive<br/>â€¢ Business context<br/>â€¢ Compliance tracking]
    end
    
    J1 --> K1
    J2 --> K1
    
    K5 --> L1
    K5 --> L2
    K5 --> L3
    
    style J1 fill:#e0f2f1
    style J2 fill:#e0f2f1
    style L1 fill:#fce4ec
    style L2 fill:#fce4ec
    style L3 fill:#fce4ec
```

### 4.5 Logic Analyzer Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        M1[COBOL Programs<br/>â€¢ Business logic chunks<br/>â€¢ Conditional statements<br/>â€¢ Calculation rules]
        M2[JCL Job Flows<br/>â€¢ Step dependencies<br/>â€¢ Control statements<br/>â€¢ Error handling]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        N1[ğŸ” Pattern Detection<br/>â€¢ Identify IF-THEN logic<br/>â€¢ Extract calculations<br/>â€¢ Find validation rules<br/>â€¢ Map control flow] --> N2[ğŸ§® Complexity Analysis<br/>â€¢ Calculate cyclomatic complexity<br/>â€¢ Assess nesting levels<br/>â€¢ Count decision points<br/>â€¢ Evaluate maintainability] --> N3[ğŸ§  API Logic Extraction<br/>â€¢ Extract business rules<br/>â€¢ Identify optimization opportunities<br/>â€¢ Generate explanations<br/>â€¢ Document processes] --> N4[ğŸ“Š Quality Assessment<br/>â€¢ Code quality metrics<br/>â€¢ Performance analysis<br/>â€¢ Risk identification<br/>â€¢ Best practice evaluation] --> N5[ğŸ“‹ Recommendation Engine<br/>â€¢ Generate improvements<br/>â€¢ Suggest refactoring<br/>â€¢ Identify technical debt<br/>â€¢ Prioritize changes]
    end
    
    subgraph "Outputs (Bottom Right)"
        O1[ğŸ“Š Logic Analysis<br/>â€¢ 15 business rules found<br/>â€¢ Complexity score: 6.2/10<br/>â€¢ 3 optimization opportunities]
        
        O2[ğŸ”§ Recommendations<br/>â€¢ Refactor 3 high-complexity methods<br/>â€¢ Add error handling<br/>â€¢ Optimize loops]
        
        O3[ğŸ“ˆ Quality Metrics<br/>â€¢ Maintainability: 7.5/10<br/>â€¢ Code quality: 8.1/10<br/>â€¢ Technical debt: Medium]
    end
    
    M1 --> N1
    M2 --> N1
    
    N5 --> O1
    N5 --> O2
    N5 --> O3
    
    style M1 fill:#f1f8e9
    style M2 fill:#f1f8e9
    style O1 fill:#e3f2fd
    style O2 fill:#e3f2fd
    style O3 fill:#e3f2fd
```

### 4.6 Documentation Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        P1[Analysis Results<br/>â€¢ Logic analysis data<br/>â€¢ Lineage mappings<br/>â€¢ Code complexity metrics]
        P2[Business Context<br/>â€¢ Field classifications<br/>â€¢ Process descriptions<br/>â€¢ Compliance requirements]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        Q1[ğŸ“‹ Content Aggregation<br/>â€¢ Collect analysis results<br/>â€¢ Merge related data<br/>â€¢ Organize by component<br/>â€¢ Structure information] --> Q2[ğŸ§  API Documentation<br/>â€¢ Generate descriptions<br/>â€¢ Create summaries<br/>â€¢ Explain processes<br/>â€¢ Add context] --> Q3[ğŸ“ Format Generation<br/>â€¢ Create markdown docs<br/>â€¢ Generate HTML reports<br/>â€¢ Build cross-references<br/>â€¢ Add navigation] --> Q4[ğŸ¨ Enhancement<br/>â€¢ Add diagrams<br/>â€¢ Include examples<br/>â€¢ Create glossaries<br/>â€¢ Improve readability] --> Q5[âœ… Validation<br/>â€¢ Check completeness<br/>â€¢ Verify accuracy<br/>â€¢ Ensure consistency<br/>â€¢ Final review]
    end
    
    subgraph "Outputs (Bottom Right)"
        R1[ğŸ“š Technical Documentation<br/>â€¢ 50+ pages generated<br/>â€¢ Component descriptions<br/>â€¢ Process flows]
        
        R2[ğŸ“Š Executive Reports<br/>â€¢ System overview<br/>â€¢ Risk assessments<br/>â€¢ Recommendations]
        
        R3[ğŸ”— Interactive Docs<br/>â€¢ Searchable content<br/>â€¢ Cross-referenced<br/>â€¢ Hyperlinked navigation]
    end
    
    P1 --> Q1
    P2 --> Q1
    
    Q5 --> R1
    Q5 --> R2
    Q5 --> R3
    
    style P1 fill:#f9fbe7
    style P2 fill:#f9fbe7
    style R1 fill:#ede7f6
    style R2 fill:#ede7f6
    style R3 fill:#ede7f6
```

### 4.7 Chat Agent Flow

```mermaid
flowchart LR
    subgraph "Inputs (Top Left)"
        S1[User Question<br/>How does the system<br/>handle large sell orders?]
        S2[Context Data<br/>â€¢ Conversation history<br/>â€¢ Available analysis<br/>â€¢ System knowledge]
    end
    
    subgraph "Processing Workflow (Center - Horizontal)"
        T1[ğŸ§© Query Classification<br/>â€¢ Identify intent type<br/>â€¢ Extract components<br/>â€¢ Determine complexity<br/>â€¢ Plan response strategy] --> T2[ğŸ” Context Gathering<br/>â€¢ Search vector index<br/>â€¢ Query databases<br/>â€¢ Get analysis results<br/>â€¢ Collect relevant data] --> T3[ğŸ§  API Response Generation<br/>â€¢ Generate explanation<br/>â€¢ Add technical details<br/>â€¢ Include examples<br/>â€¢ Provide guidance] --> T4[ğŸ’¡ Enhancement<br/>â€¢ Add suggestions<br/>â€¢ Include references<br/>â€¢ Format response<br/>â€¢ Ensure clarity] --> T5[âœ… Quality Check<br/>â€¢ Validate accuracy<br/>â€¢ Check completeness<br/>â€¢ Ensure helpfulness<br/>â€¢ Final formatting]
    end
    
    subgraph "Outputs (Bottom Right)"
        U1[ğŸ’¬ Intelligent Response<br/>Large sell orders trigger<br/>LARGE-ORDER-CHECK<br/>validation in SECTRD01]
        
        U2[ğŸ“‹ Follow-up Suggestions<br/>â€¢ Show validation logic<br/>â€¢ Analyze risk controls<br/>â€¢ Find similar patterns]
        
        U3[ğŸ”— Context Links<br/>â€¢ Related components<br/>â€¢ Additional resources<br/>â€¢ Cross-references]
    end
    
    S1 --> T1
    S2 --> T1
    
    T5 --> U1
    T5 --> U2
    T5 --> U3
    
    style S1 fill:#e1f5fe
    style S2 fill:#e1f5fe
    style U1 fill:#e8f5e8
    style U2 fill:#e8f5e8
    style U3 fill:#e8f5e8
```

---

## 5. Agent Coordination Flow

```mermaid
sequenceDiagram
    participant User
    participant Coordinator
    participant CodeParser
    participant DataLoader
    participant VectorIndex
    participant Lineage
    participant Logic
    participant Docs
    participant Chat
    participant GPU_API
    
    User->>Coordinator: Upload security_trading.cbl
    Coordinator->>CodeParser: Process COBOL file
    CodeParser->>GPU_API: Analyze business logic
    GPU_API-->>CodeParser: Business rule extraction
    CodeParser-->>Coordinator: 250 code chunks created
    
    Coordinator->>DataLoader: Process transaction.csv
    DataLoader->>GPU_API: Generate field descriptions
    GPU_API-->>DataLoader: Enhanced schema
    DataLoader-->>Coordinator: Tables created, data loaded
    
    Coordinator->>VectorIndex: Index all chunks
    VectorIndex->>VectorIndex: Generate embeddings (local)
    VectorIndex-->>Coordinator: FAISS index ready
    
    User->>Chat: "Analyze CUSTOMER-ID lineage"
    Chat->>Coordinator: Request lineage analysis
    Coordinator->>Lineage: Analyze CUSTOMER-ID
    Lineage->>GPU_API: Analyze field usage patterns
    GPU_API-->>Lineage: Usage context analysis
    Lineage-->>Chat: Lineage map with 15 programs
    Chat->>GPU_API: Generate response
    GPU_API-->>Chat: Natural language explanation
    Chat-->>User: "CUSTOMER-ID flows through..."
```

---

## 6. Output Artifacts

The Opulence system produces these deliverables for the bank's security trading system:

âœ… **Field-level data lineage reports**  
   - "CUSTOMER-ID flows from CUSTMAST â†’ SECTRD01 â†’ PORTFOLIO-UPDATE â†’ TRADE-HISTORY"
   - Compliance-ready audit trails

âœ… **Extracted business logic summaries**  
   - "Stop-loss orders: IF CURRENT-PRICE < (STOP-PRICE * 0.95) THEN EXECUTE-SELL"
   - Trading rule documentation in plain English

âœ… **Annotated markdown documentation of code modules**  
   - Complete explanation of settlement processing
   - Cross-references between related programs

âœ… **Interactive chat interface for querying understanding**  
   - "What happens when a trade fails settlement?"
   - "Show me all programs that update customer portfolios"

---

## 7. Sample Data Context: Private Wealth Bank Security Transactions

### Input Files for Analysis:

**COBOL Programs:**
- `SECTRD01.cbl` - Main security trading program (2,500 lines)
- `VALIDATE.cbl` - Order validation logic (800 lines)  
- `SETTLE.cbl` - Settlement processing (1,200 lines)
- `PORTFOLIO.cbl` - Portfolio update logic (900 lines)

**JCL Jobs:**
- `DAILYTRD.jcl` - Daily trade processing batch job
- `SETTLEMENT.jcl` - End-of-day settlement job
- `RECON.jcl` - Trade reconciliation job

**DB2 Tables:**
```sql
-- SECURITY_TRANSACTION table
CREATE TABLE SECURITY_TXN (
    CUST_ID        CHAR(10),
    TRADE_ID       CHAR(15),
    SECURITY_CODE  CHAR(8),
    TRADE_TYPE     CHAR(4),    -- BUY/SELL
    QUANTITY       DECIMAL(15,2),
    PRICE          DECIMAL(15,4),
    TRADE_DATE     DATE,
    SETTLE_DATE    DATE,
    STATUS         CHAR(3)     -- PEN/SET/FAI
);
```

**Sample Transaction Data:**
```csv
CUST_ID,TRADE_ID,SECURITY_CODE,TRADE_TYPE,QUANTITY,PRICE,TRADE_DATE,STATUS
PWB0001234,TRD20241201001,AAPL,BUY,100,150.25,2024-12-01,PEN
PWB0001234,TRD20241201002,TSLA,SELL,50,245.80,2024-12-01,SET
PWB0001567,TRD20241201003,MSFT,BUY,200,380.15,2024-12-01,FAI
```

---

## 8. Individual Agent Explanations

### Vector Index Agent
**Purpose**: Creates searchable embeddings of all code segments and business logic.

**Bank Example**: When analyzing the security trading system, this agent:
- Embeds all COBOL paragraphs dealing with order validation
- Creates vectors for trading rule conditions  
- Enables semantic search like "find all margin calculation logic"

**API Integration**: Makes HTTP calls to CodeLLaMA to generate embeddings and understand code semantics.

### Lineage Agent  
**Purpose**: Tracks how data fields flow through the entire system.

**Bank Example**: For a customer security purchase:
1. **CUSTOMER-ID** enters via online trading platform
2. Flows through `VALIDATE.cbl` for KYC checks
3. Processed in `SECTRD01.cbl` for order execution
4. Updates `PORTFOLIO.cbl` for position management
5. Records in `TRADE-HISTORY` table for audit

**Critical for Compliance**: Regulators require complete audit trails showing how customer data is processed.

### Logic Analyzer Agent
**Purpose**: Extracts and explains complex business rules embedded in COBOL logic.

**Bank Example**: Discovers trading rules like:
```cobol
IF TRADE-AMOUNT > DAILY-LIMIT
   AND CUSTOMER-TIER NOT = 'PLATINUM'
   THEN MOVE 'HOLD' TO TRADE-STATUS
   PERFORM MANUAL-APPROVAL-PROCESS
```

Translates to: "Trades over daily limit require manual approval unless customer is Platinum tier."

### Documentation Agent
**Purpose**: Creates human-readable documentation explaining system functionality.

**Bank Example**: Generates documentation like:
- "Settlement Process Overview: How T+2 settlement works"
- "Stop-Loss Order Processing: Automated selling when price thresholds are breached"
- "Customer Portfolio Updates: Real-time vs. batch processing logic"

### Chat Agent
**Purpose**: Provides conversational interface for querying system knowledge.

**Bank Example Queries**:
- "How does the system handle partial fills on large orders?"
- "What validation checks are performed before executing a trade?"
- "Show me the settlement process for international securities"

**Response Example**: "When a large order cannot be filled completely, the PARTIAL-FILL-HANDLER in SECTRD01 splits it into smaller chunks and processes them separately, updating the customer's available cash after each partial execution..."

---

## 9. Coordination Flow: Processing a Security Transaction

### Real-World Scenario: Customer Places $500K Apple Stock Purchase

1. **File Processing Phase**:
   - Code Parser analyzes `SECTRD01.cbl` and extracts order processing logic
   - Data Loader imports recent Apple trading data and customer portfolio info
   - System identifies all programs involved in large order processing

2. **Analysis Phase**:
   - **Vector Index Agent**: Finds all code segments related to large order handling
   - **Lineage Agent**: Maps how customer cash balance flows through the system
   - **Logic Analyzer**: Extracts validation rules for large orders (credit checks, position limits)
   - **Documentation Agent**: Summarizes the complete order-to-settlement workflow

3. **Query Phase**:
   - Risk manager asks: "What approvals are needed for this trade size?"
   - Chat Agent searches indexed knowledge and responds: "Orders over $250K require senior trader approval per LARGE-ORDER-CHECK paragraph, plus real-time margin calculation..."

4. **Compliance Phase**:
   - Lineage reports show complete audit trail
   - Logic summaries document all decision points
   - Documentation provides regulatory-compliant process descriptions

This architecture transforms decades-old, undocumented mainframe code into an accessible, searchable knowledge base that supports both operational teams and regulatory compliance requirements.

---

## 10. Technical Implementation Notes

### API-Based Architecture
The Opulence system uses HTTP APIs to communicate with GPU-hosted CodeLLaMA models, enabling:
- **Scalability**: Multiple model servers can handle concurrent analysis requests
- **Load Balancing**: Requests are distributed across available GPU resources
- **Fault Tolerance**: Circuit breakers and retry logic ensure robust operation
- **Resource Efficiency**: No need for local GPU allocation per agent

### Database Design
SQLite database stores:
- **program_chunks**: Parsed code segments with metadata
- **field_lineage**: Data flow tracking for compliance
- **vector_embeddings**: FAISS index references for semantic search
- **processing_stats**: Performance monitoring and audit trails

This architecture enables financial institutions to understand and maintain critical legacy systems while meeting modern regulatory and operational requirements.